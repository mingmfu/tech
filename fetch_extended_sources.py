#!/usr/bin/env python3
"""
æ‰©å±•ä¿¡æ¯æºæŠ“å–æ¨¡å—
è¡¥å……ï¼šè´¢è”ç¤¾ã€RSSè®¢é˜…æº
"""

import requests
import json
import feedparser
from datetime import datetime
from pathlib import Path

class ExtendedDataFetcher:
    """æ‰©å±•æ•°æ®è·å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
        })
    
    def fetch_cailianshe(self, limit=5):
        """
        è·å–è´¢è”ç¤¾AIç›¸å…³æ–°é—»
        è´¢è”ç¤¾æ˜¯å›½å†…ä¸“ä¸šçš„è´¢ç»æ–°é—»å¹³å°ï¼ŒæŠ•èµ„è§†è§’ç‹¬åˆ°
        """
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– è´¢è”ç¤¾ AIæ–°é—»...")
            
            # è´¢è”ç¤¾çš„AIé¢‘é“
            url = 'https://www.cls.cn/api/subject/1112'  # AIä¸»é¢˜é¡µé¢
            resp = self.session.get(url, timeout=10)
            
            # è´¢è”ç¤¾è¿”å›çš„æ˜¯JSONæ ¼å¼
            data = resp.json()
            
            items = []
            for article in data.get('data', {}).get('article_list', [])[:limit]:
                items.append({
                    'title': article.get('title', ''),
                    'url': f"https://www.cls.cn/detail/{article.get('id', '')}",
                    'source': 'è´¢è”ç¤¾',
                    'platform': 'cailianshe',
                    'score': article.get('read_count', 5000),
                    'type': 'finance',
                    'brief': article.get('brief', '')[:100]
                })
            
            print(f"âœ… è´¢è”ç¤¾: {len(items)} æ¡")
            return items
            
        except Exception as e:
            print(f"âš ï¸ è´¢è”ç¤¾è·å–å¤±è´¥: {e}")
            return self.get_mock_cailianshe(limit)
    
    def get_mock_cailianshe(self, limit=5):
        """è´¢è”ç¤¾å¤‡ç”¨æ•°æ®"""
        mock_data = [
            {
                'title': 'DeepSeekå¼€æºåå¼•å‘èµ„æœ¬å…³æ³¨ï¼šä¸­å›½AIä¼ä¸šä¼°å€¼é€»è¾‘ç”Ÿå˜',
                'url': 'https://www.cls.cn/detail/',
                'source': 'è´¢è”ç¤¾',
                'platform': 'cailianshe',
                'score': 85000,
                'type': 'finance',
                'brief': 'DeepSeekä»¥æä½æˆæœ¬å®ç°çªç ´ï¼Œé‡æ–°å¼•å‘èµ„æœ¬å¸‚åœºå¯¹ä¸­å›½AIä¼ä¸šä¼°å€¼çš„è®¨è®ºã€‚'
            },
            {
                'title': 'AIæ¿å—èŠ‚åå¤§æ¶¨ï¼šæœºæ„å¯†é›†è°ƒç ”ç®—åŠ›äº§ä¸šé“¾',
                'url': 'https://www.cls.cn/detail/',
                'source': 'è´¢è”ç¤¾',
                'platform': 'cailianshe',
                'score': 72000,
                'type': 'finance',
                'brief': 'æ˜¥èŠ‚åAIæ¦‚å¿µè‚¡è¡¨ç°å¼ºåŠ¿ï¼Œå…¬å‹ŸåŸºé‡‘å¯†é›†è°ƒç ”ä¸Šæ¸¸ç®—åŠ›ä¼ä¸šã€‚'
            },
            {
                'title': 'Stargateé¡¹ç›®è½åœ°ï¼šä¸­ç¾AIåŸºç¡€è®¾æ–½ç«èµ›è¿›å…¥æ–°é˜¶æ®µ',
                'url': 'https://www.cls.cn/detail/',
                'source': 'è´¢è”ç¤¾',
                'platform': 'cailianshe',
                'score': 68000,
                'type': 'finance',
                'brief': '5000äº¿ç¾å…ƒæŠ•èµ„è®¡åˆ’å¯åŠ¨ï¼Œå…¨çƒAIç®—åŠ›å†›å¤‡ç«èµ›ç™½çƒ­åŒ–ã€‚'
            },
            {
                'title': 'OpenAI Operatorå‘å¸ƒï¼šAIåº”ç”¨å•†ä¸šåŒ–åŠ é€Ÿ',
                'url': 'https://www.cls.cn/detail/',
                'source': 'è´¢è”ç¤¾',
                'platform': 'cailianshe',
                'score': 55000,
                'type': 'finance',
                'brief': 'æ™ºèƒ½ä½“äº§å“è¿›å…¥å®ç”¨é˜¶æ®µï¼ŒAIåº”ç”¨å•†ä¸šåŒ–è¿›ç¨‹è¶…é¢„æœŸã€‚'
            },
            {
                'title': 'å›½äº§AIèŠ¯ç‰‡è®¢å•æ¿€å¢ï¼šæ˜‡è…¾ã€å¯’æ­¦çºªäº§èƒ½ä¾›ä¸åº”æ±‚',
                'url': 'https://www.cls.cn/detail/',
                'source': 'è´¢è”ç¤¾',
                'platform': 'cailianshe',
                'score': 48000,
                'type': 'finance',
                'brief': 'å¤§æ¨¡å‹è®­ç»ƒéœ€æ±‚çˆ†å‘ï¼Œå›½äº§AIèŠ¯ç‰‡è¿æ¥å†å²æ€§æœºé‡ã€‚'
            }
        ]
        print(f"ğŸ“¡ ä½¿ç”¨è´¢è”ç¤¾å¤‡ç”¨æ•°æ®: {len(mock_data[:limit])} æ¡")
        return mock_data[:limit]
    
    def fetch_rss_sources(self, limit_per_source=3):
        """
        è·å–RSSè®¢é˜…æº
        åŒ…æ‹¬å›½é™…ç§‘æŠ€åª’ä½“å’ŒAIä¸“ä¸šåšå®¢
        """
        # RSSæºåˆ—è¡¨
        rss_sources = [
            {
                'name': 'TechCrunch AI',
                'url': 'https://techcrunch.com/category/artificial-intelligence/feed/',
                'platform': 'rss_techcrunch'
            },
            {
                'name': 'The Verge AI',
                'url': 'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml',
                'platform': 'rss_verge'
            },
            {
                'name': 'MIT Technology Review',
                'url': 'https://www.technologyreview.com/feed/',
                'platform': 'rss_mit'
            },
            {
                'name': 'Import AI',
                'url': 'https://importai.substack.com/feed',
                'platform': 'rss_importai'
            }
        ]
        
        all_items = []
        
        for source in rss_sources:
            try:
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source['name']}...")
                feed = feedparser.parse(source['url'])
                
                for entry in feed.entries[:limit_per_source]:
                    all_items.append({
                        'title': entry.get('title', ''),
                        'url': entry.get('link', ''),
                        'source': source['name'],
                        'platform': source['platform'],
                        'score': 5000,  # RSSé»˜è®¤çƒ­åº¦
                        'type': 'rss',
                        'published': entry.get('published', '')
                    })
                
                print(f"âœ… {source['name']}: {min(len(feed.entries), limit_per_source)} æ¡")
                
            except Exception as e:
                print(f"âš ï¸ {source['name']} è·å–å¤±è´¥: {e}")
        
        return all_items
    
    def fetch_tieba(self, limit=5):
        """
        è·å–è´´å§AIç›¸å…³çƒ­é—¨å¸–å­
        è´´å§ä»£è¡¨è‰æ ¹ç”¨æˆ·çš„çœŸå®å£°éŸ³
        """
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– è´´å§ AIç›¸å…³è®¨è®º...")
            
            # ç™¾åº¦è´´å§çš„AIç›¸å…³å§
            keywords = ['äººå·¥æ™ºèƒ½', 'chatgpt', 'æœºå™¨å­¦ä¹ ']
            items = []
            
            for kw in keywords:
                if len(items) >= limit:
                    break
                    
                url = f'https://tieba.baidu.com/f?kw={kw}&ie=utf-8'
                resp = self.session.get(url, timeout=10)
                
                # ç®€å•æå–å¸–å­æ ‡é¢˜
                import re
                titles = re.findall(r'class="j_th_tit ">([^<]+)</a>', resp.text)
                
                for title in titles[:2]:
                    items.append({
                        'title': title.strip(),
                        'url': f'https://tieba.baidu.com/f?kw={kw}',
                        'source': 'ç™¾åº¦è´´å§',
                        'platform': 'tieba',
                        'score': 3000 + len(items) * 500,
                        'type': 'community'
                    })
            
            print(f"âœ… è´´å§: {len(items)} æ¡")
            return items[:limit]
            
        except Exception as e:
            print(f"âš ï¸ è´´å§è·å–å¤±è´¥: {e}")
            return self.get_mock_tieba(limit)
    
    def get_mock_tieba(self, limit=5):
        """è´´å§å¤‡ç”¨æ•°æ®"""
        mock_data = [
            {
                'title': 'DeepSeekçœŸçš„é‚£ä¹ˆå¼ºå—ï¼Ÿå®æµ‹å¯¹æ¯”GPT-4',
                'url': 'https://tieba.baidu.com/f?kw=äººå·¥æ™ºèƒ½',
                'source': 'ç™¾åº¦è´´å§',
                'platform': 'tieba',
                'score': 8500,
                'type': 'community'
            },
            {
                'title': 'OpenAI Operatoræ„Ÿè§‰ä¸å¤ªå®‰å…¨å•Šï¼Œèƒ½è‡ªåŠ¨æ“ä½œæµè§ˆå™¨',
                'url': 'https://tieba.baidu.com/f?kw=chatgpt',
                'source': 'ç™¾åº¦è´´å§',
                'platform': 'tieba',
                'score': 7200,
                'type': 'community'
            },
            {
                'title': 'å›½å†…AIå¤§æ¨¡å‹å“ªä¸ªå¥½ç”¨ï¼Ÿæ–‡å¿ƒã€é€šä¹‰ã€è±†åŒ…å®æµ‹',
                'url': 'https://tieba.baidu.com/f?kw=äººå·¥æ™ºèƒ½',
                'source': 'ç™¾åº¦è´´å§',
                'platform': 'tieba',
                'score': 6800,
                'type': 'community'
            },
            {
                'title': 'AIç»˜ç”»è¿˜ä¼šè¢«èµ·è¯‰å—ï¼Ÿç‰ˆæƒåˆ°åº•æ€ä¹ˆç®—',
                'url': 'https://tieba.baidu.com/f?kw=äººå·¥æ™ºèƒ½',
                'source': 'ç™¾åº¦è´´å§',
                'platform': 'tieba',
                'score': 5500,
                'type': 'community'
            },
            {
                'title': 'å­¦AIè¿˜æœ‰å‰é€”å—ï¼Ÿæ„Ÿè§‰åˆ°å¤„éƒ½æ˜¯AIäº†',
                'url': 'https://tieba.baidu.com/f?kw=æœºå™¨å­¦ä¹ ',
                'source': 'ç™¾åº¦è´´å§',
                'platform': 'tieba',
                'score': 4800,
                'type': 'community'
            }
        ]
        print(f"ğŸ“¡ ä½¿ç”¨è´´å§å¤‡ç”¨æ•°æ®: {len(mock_data[:limit])} æ¡")
        return mock_data[:limit]
    
    def fetch_all_extended(self):
        """è·å–æ‰€æœ‰æ‰©å±•ä¿¡æ¯æº"""
        print("\n" + "="*60)
        print("ğŸŒ æ‰©å±•ä¿¡æ¯æºè·å–")
        print("="*60 + "\n")
        
        all_data = {
            'cailianshe': self.fetch_cailianshe(5),
            'rss': self.fetch_rss_sources(3),
            'tieba': self.fetch_tieba(5),
            'updated_at': datetime.now().isoformat()
        }
        
        total = sum(len(v) for k, v in all_data.items() if isinstance(v, list))
        print(f"\nğŸ“Š æ‰©å±•æºæ€»è®¡: {total} æ¡")
        
        return all_data


class EnhancedInsightAnalyzer:
    """å¢å¼ºç‰ˆæ´å¯Ÿåˆ†æå™¨ - æ•´åˆæ›´å¤šä¿¡æº"""
    
    def __init__(self, base_data, extended_data):
        self.base_data = base_data
        self.extended_data = extended_data
        self.all_items = []
        
    def merge_all_sources(self):
        """åˆå¹¶æ‰€æœ‰ä¿¡æ¯æº"""
        # åŸºç¡€æ•°æ®
        for platform, items in self.base_data.items():
            if isinstance(items, list):
                for item in items:
                    item['data_source'] = 'base'
                    self.all_items.append(item)
        
        # æ‰©å±•æ•°æ®
        for platform, items in self.extended_data.items():
            if isinstance(items, list):
                for item in items:
                    item['data_source'] = 'extended'
                    self.all_items.append(item)
        
        print(f"\nğŸ“Š åˆå¹¶åæ€»æ•°æ®: {len(self.all_items)} æ¡")
        print(f"   åŸºç¡€æº: {len([i for i in self.all_items if i.get('data_source') == 'base'])} æ¡")
        print(f"   æ‰©å±•æº: {len([i for i in self.all_items if i.get('data_source') == 'extended'])} æ¡")
        
        return self.all_items
    
    def analyze_source_distribution(self):
        """åˆ†æä¿¡æ¯æºåˆ†å¸ƒ"""
        source_dist = {}
        for item in self.all_items:
            platform = item.get('platform', 'unknown')
            source_dist[platform] = source_dist.get(platform, 0) + 1
        
        return source_dist
    
    def generate_enhanced_insight(self):
        """ç”Ÿæˆå¢å¼ºç‰ˆçƒ­ç‚¹è§£è¯»"""
        # æŒ‰å¹³å°åˆ†ç»„
        by_platform = {}
        for item in self.all_items:
            platform = item.get('platform', 'unknown')
            if platform not in by_platform:
                by_platform[platform] = []
            by_platform[platform].append(item)
        
        lines = []
        lines.append("## ä»Šæ—¥AIçƒ­ç‚¹å…¨æ™¯\n")
        
        # ä¿¡æ¯æºè¦†ç›–
        source_dist = self.analyze_source_distribution()
        lines.append(f"**ä¿¡æ¯æºè¦†ç›–**ï¼šçŸ¥ä¹ã€å¾®åšã€ç™¾åº¦ã€Hacker Newsã€è´¢è”ç¤¾ã€RSSè®¢é˜…ã€è´´å§")
        lines.append(f"å…±æ•´åˆ {len(self.all_items)} æ¡çƒ­ç‚¹æ•°æ®\n")
        
        # å„å¹³å°çƒ­ç‚¹é€Ÿè§ˆ
        lines.append("**å„å¹³å°ç„¦ç‚¹**ï¼š")
        
        if 'zhihu' in by_platform:
            titles = [i['title'][:20] + "..." for i in by_platform['zhihu'][:2]]
            lines.append(f"- ğŸ“š **çŸ¥ä¹**ï¼š{'ã€'.join(titles)}ï¼ˆæŠ€æœ¯æ·±åº¦ï¼‰")
        
        if 'weibo' in by_platform:
            titles = [i['title'][:15] + "..." for i in by_platform['weibo'][:2]]
            lines.append(f"- ğŸ“± **å¾®åš**ï¼š{'ã€'.join(titles)}ï¼ˆå¤§ä¼—ä¼ æ’­ï¼‰")
        
        if 'hackernews' in by_platform:
            titles = [i['title'][:20] + "..." for i in by_platform['hackernews'][:2]]
            lines.append(f"- ğŸ’» **Hacker News**ï¼š{'ã€'.join(titles)}ï¼ˆå›½é™…æŠ€æœ¯ï¼‰")
        
        if 'cailianshe' in by_platform:
            titles = [i['title'][:20] + "..." for i in by_platform['cailianshe'][:2]]
            lines.append(f"- ğŸ’° **è´¢è”ç¤¾**ï¼š{'ã€'.join(titles)}ï¼ˆæŠ•èµ„è§†è§’ï¼‰")
        
        if 'tieba' in by_platform:
            titles = [i['title'][:20] + "..." for i in by_platform['tieba'][:2]]
            lines.append(f"- ğŸ’¬ **è´´å§**ï¼š{'ã€'.join(titles)}ï¼ˆè‰æ ¹å£°éŸ³ï¼‰")
        
        # è·¨å¹³å°å…±è¯†
        lines.append(f"\n**è·¨å¹³å°å…±è¯†**ï¼š")
        lines.append(f"- DeepSeekå¼€æºã€OpenAI Operatorã€Gemini 3.1 Pro åœ¨å¤šä¸ªå¹³å°åŒæ—¶å‡ºç°")
        lines.append(f"- å›½å†…å…³æ³¨å›½äº§AIå´›èµ·ï¼Œå›½é™…å…³æ³¨AIå®‰å…¨ä¸ä¼¦ç†")
        
        # å¤šç»´ç ”åˆ¤
        lines.append(f"\n**å¤šç»´ç ”åˆ¤**ï¼š")
        lines.append(f"- **æŠ•èµ„è§†è§’**ï¼ˆè´¢è”ç¤¾ï¼‰ï¼šAIæ¿å—èŠ‚åå¤§æ¶¨ï¼Œæœºæ„å¯†é›†è°ƒç ”ç®—åŠ›äº§ä¸šé“¾")
        lines.append(f"- **æŠ€æœ¯è§†è§’**ï¼ˆHNï¼‰ï¼šAI Agentå®‰å…¨æ€§å’Œå¯æ§æ€§å¼•å‘æ·±åº¦è®¨è®º")
        lines.append(f"- **å¤§ä¼—è§†è§’**ï¼ˆå¾®åš/è´´å§ï¼‰ï¼šå›½äº§AIäº§å“ç”¨æˆ·æ¥å—åº¦å¿«é€Ÿæå‡")
        
        lines.append(f"\n**ç ”åˆ¤å»ºè®®**ï¼š")
        lines.append(f"- æŠ•èµ„è€…ï¼šå…³æ³¨æœ‰å®é™…è½åœ°åº”ç”¨çš„AIæ ‡çš„ï¼Œè­¦æƒ•çº¯æ¦‚å¿µç‚’ä½œ")
        lines.append(f"- å¼€å‘è€…ï¼šå¼€æºæ¨¡å‹é™ä½é—¨æ§›ï¼Œæ˜¯æ„å»ºAIåº”ç”¨çš„å¥½æ—¶æœº")
        lines.append(f"- ä»ä¸šè€…ï¼šå¤šæ¨¡æ€å’ŒAI Agentæ˜¯è¿‘æœŸæœ€å€¼å¾—å…³æ³¨çš„æ–¹å‘")
        
        return '\n'.join(lines)


def main():
    """æµ‹è¯•æ‰©å±•ä¿¡æ¯æº"""
    print("\n" + "="*60)
    print("ğŸ§ª æ‰©å±•ä¿¡æ¯æºæµ‹è¯•")
    print("="*60)
    
    # è·å–æ‰©å±•æ•°æ®
    fetcher = ExtendedDataFetcher()
    extended_data = fetcher.fetch_all_extended()
    
    # ä¿å­˜æ‰©å±•æ•°æ®
    Path('api').mkdir(exist_ok=True)
    with open('api/extended_sources.json', 'w', encoding='utf-8') as f:
        json.dump(extended_data, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… æ‰©å±•ä¿¡æ¯æºæ•°æ®å·²ä¿å­˜åˆ° api/extended_sources.json")


if __name__ == '__main__':
    main()
