#!/usr/bin/env python3
"""
TechInsight Hub - å¤šå¹³å°çƒ­æ¦œèšåˆä¸AIåˆ†æç³»ç»Ÿ v2.1
æ”¯æŒï¼šçŸ¥ä¹ã€å¾®åšã€Hacker Newsã€ç™¾åº¦çƒ­æœã€è´¢è”ç¤¾ + å¤‡ç”¨æ•°æ®
"""

import requests
import json
import re
import time
from datetime import datetime
from pathlib import Path

class TrendingFetcher:
    """å¤šå¹³å°çƒ­æ¦œè·å–å™¨"""
    
    def __init__(self, use_mock=False):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        self.use_mock = use_mock
    
    # å¤‡ç”¨æ•°æ® - å½“æŠ“å–å¤±è´¥æ—¶ä½¿ç”¨
    MOCK_ZHIHU = [
        {"title": "DeepSeek-R1æ¨ç†æ¨¡å‹æŠ€æœ¯æŠ¥å‘Šå…¬å¼€ï¼šå¦‚ä½•ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›", "score": "580ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "tech"},
        {"title": "Google Gemini 3.1 Proå‘å¸ƒï¼Œå¤šæ¨¡æ€èƒ½åŠ›å†å‡çº§ï¼Œèƒ½å¦æŒ‘æˆ˜GPT-4åœ°ä½ï¼Ÿ", "score": "420ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "tech"},
        {"title": "OpenAI Operatoræ™ºèƒ½ä½“å¼•å‘çƒ­è®®ï¼šAIèƒ½è‡ªä¸»æ“ä½œæµè§ˆå™¨æ˜¯å¥½äº‹è¿˜æ˜¯é£é™©ï¼Ÿ", "score": "380ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "discussion"},
        {"title": "å›½å†…AIå¤§æ¨¡å‹ä»·æ ¼æˆ˜æŒç»­ï¼šè°èƒ½åœ¨è¿™åœºçƒ§é’±å¤§æˆ˜ä¸­ç¬‘åˆ°æœ€åï¼Ÿ", "score": "290ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "discussion"},
        {"title": "Stargateé¡¹ç›®5000äº¿ç¾å…ƒæŠ•èµ„ï¼šç¾å›½AIåŸºç¡€è®¾æ–½å»ºè®¾çš„è±ªèµŒ", "score": "250ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "news"},
        {"title": "AI Agentå†™é”™æŠ¥é“å¼•å‘çš„æ€è€ƒï¼šAIæ–°é—»ä¸šçš„ä¼¦ç†è¾¹ç•Œåœ¨å“ªé‡Œï¼Ÿ", "score": "180ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "discussion"},
        {"title": "ä»ChatGPTåˆ°Operatorï¼šOpenAIçš„äº§å“æ¼”è¿›è·¯çº¿é€éœ²äº†ä»€ä¹ˆæˆ˜ç•¥æ„å›¾ï¼Ÿ", "score": "165ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "analysis"},
        {"title": "å¾®è½¯AIç¨‹åºå‘˜è¢«è£åå‘å£°ï¼šæˆ‘ä»¬è®­ç»ƒAIå–ä»£äº†è‡ªå·±", "score": "140ä¸‡", "url": "https://zhuanlan.zhihu.com/p/", "type": "news"},
    ]
    
    MOCK_WEIBO = [
        {"title": "DeepSeekå¼€æºå¤§æ¨¡å‹éœ‡æ’¼ç¡…è°·", "score": 12500000, "category": "ç§‘æŠ€"},
        {"title": "å›½äº§AIæœºå™¨äººæ˜¥æ™šä¸Šæ¼”èˆè¹ˆ", "score": 9800000, "category": "ç§‘æŠ€"},
        {"title": "OpenAIå‘å¸ƒOperatoræ™ºèƒ½ä½“", "score": 8200000, "category": "ç§‘æŠ€"},
        {"title": "GPT-5 rumored to be training", "score": 6500000, "category": "ç§‘æŠ€"},
        {"title": "å­—èŠ‚è±†åŒ…å¤§æ¨¡å‹ç”¨æˆ·ç ´åƒä¸‡", "score": 5400000, "category": "ç§‘æŠ€"},
        {"title": "è°·æ­ŒGemini 3.1 Proå‘å¸ƒ", "score": 4800000, "category": "ç§‘æŠ€"},
        {"title": "é©¬æ–¯å…‹ç§°Grok 3å°†æ˜¯æœ€å¼ºAI", "score": 4200000, "category": "ç§‘æŠ€"},
        {"title": "AIç»˜ç”»ç‰ˆæƒäº‰è®®å†å‡çº§", "score": 3600000, "category": "ç§‘æŠ€"},
    ]
    
    MOCK_BAIDU = [
        {"title": "DeepSeek-R1æ¨¡å‹å¼€æº", "hotScore": 4985000},
        {"title": "OpenAI OperatoråŠŸèƒ½ä»‹ç»", "hotScore": 4523000},
        {"title": "Gemini 3.1 Proå‘å¸ƒ", "hotScore": 4156000},
        {"title": "AIæ™ºèƒ½ä½“å®‰å…¨é£é™©", "hotScore": 3892000},
        {"title": "Stargateé¡¹ç›®æŠ•èµ„", "hotScore": 3567000},
        {"title": "å›½å†…å¤§æ¨¡å‹ä»·æ ¼æˆ˜", "hotScore": 3241000},
        {"title": "NVIDIA RTX 5090å‘å¸ƒ", "hotScore": 2985000},
        {"title": "AI Agentå†™é”™æŠ¥é“", "hotScore": 2654000},
    ]
    
    def fetch_zhihu(self, limit=10):
        """è·å–çŸ¥ä¹çƒ­æ¦œ"""
        if self.use_mock:
            print("ğŸ“¡ ä½¿ç”¨çŸ¥ä¹å¤‡ç”¨æ•°æ®...")
            return [{
                'title': item['title'],
                'url': item['url'],
                'source': 'çŸ¥ä¹',
                'platform': 'zhihu',
                'score': item['score'],
                'type': item.get('type', 'discussion')
            } for item in self.MOCK_ZHIHU[:limit]]
        
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– çŸ¥ä¹çƒ­æ¦œ...")
            resp = self.session.get(
                'https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=50',
                timeout=10
            )
            data = resp.json()
            
            items = []
            for item in data.get('data', [])[:limit]:
                target = item.get('target', {})
                title = target.get('title', '')
                url = target.get('url', '')
                if title and url:
                    items.append({
                        'title': title,
                        'url': url if url.startswith('http') else f"https://zhihu.com{url}",
                        'source': 'çŸ¥ä¹',
                        'platform': 'zhihu',
                        'score': item.get('detail_text', '').replace('ä¸‡', '0000').replace('çƒ­åº¦', '').strip() or '0',
                        'type': 'discussion'
                    })
            
            print(f"âœ… çŸ¥ä¹: {len(items)} æ¡")
            return items
        except Exception as e:
            print(f"âš ï¸ çŸ¥ä¹è·å–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®: {e}")
            return [{
                'title': item['title'],
                'url': item['url'],
                'source': 'çŸ¥ä¹',
                'platform': 'zhihu',
                'score': item['score'],
                'type': item.get('type', 'discussion')
            } for item in self.MOCK_ZHIHU[:limit]]
    
    def fetch_weibo(self, limit=10):
        """è·å–å¾®åšçƒ­æœ"""
        if self.use_mock:
            print("ğŸ“¡ ä½¿ç”¨å¾®åšå¤‡ç”¨æ•°æ®...")
            return [{
                'title': item['title'],
                'url': f"https://s.weibo.com/weibo?q={item['title']}",
                'source': 'å¾®åš',
                'platform': 'weibo',
                'score': str(item['score']),
                'type': 'hot',
                'category': item.get('category', '')
            } for item in self.MOCK_WEIBO[:limit]]
        
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– å¾®åšçƒ­æœ...")
            resp = self.session.get(
                'https://weibo.com/ajax/side/hotSearch',
                timeout=10
            )
            data = resp.json()
            
            items = []
            for item in data.get('data', {}).get('realtime', [])[:limit]:
                title = item.get('word', '')
                if title:
                    items.append({
                        'title': title,
                        'url': f"https://s.weibo.com/weibo?q={title}",
                        'source': 'å¾®åš',
                        'platform': 'weibo',
                        'score': str(item.get('num', 0)),
                        'type': 'hot',
                        'category': item.get('category', '')
                    })
            
            print(f"âœ… å¾®åš: {len(items)} æ¡")
            return items
        except Exception as e:
            print(f"âš ï¸ å¾®åšè·å–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®: {e}")
            return [{
                'title': item['title'],
                'url': f"https://s.weibo.com/weibo?q={item['title']}",
                'source': 'å¾®åš',
                'platform': 'weibo',
                'score': str(item['score']),
                'type': 'hot',
                'category': item.get('category', '')
            } for item in self.MOCK_WEIBO[:limit]]
    
    def fetch_hackernews(self, limit=10):
        """è·å–Hacker Newsçƒ­æ¦œ"""
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– Hacker News...")
            
            resp = self.session.get(
                'https://hacker-news.firebaseio.com/v0/topstories.json',
                timeout=10
            )
            story_ids = resp.json()[:limit+10]
            
            items = []
            for story_id in story_ids:
                if len(items) >= limit:
                    break
                try:
                    story_resp = self.session.get(
                        f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json',
                        timeout=5
                    )
                    story = story_resp.json()
                    if story and 'title' in story:
                        items.append({
                            'title': story['title'],
                            'url': story.get('url') or f"https://news.ycombinator.com/item?id={story_id}",
                            'source': 'Hacker News',
                            'platform': 'hackernews',
                            'score': story.get('score', 0),
                            'type': 'tech',
                            'comments': story.get('descendants', 0)
                        })
                    time.sleep(0.05)
                except:
                    continue
            
            print(f"âœ… Hacker News: {len(items)} æ¡")
            return items
        except Exception as e:
            print(f"âš ï¸ HNè·å–å¤±è´¥: {e}")
            return []
    
    def fetch_baidu(self, limit=10):
        """è·å–ç™¾åº¦çƒ­æœ"""
        if self.use_mock:
            print("ğŸ“¡ ä½¿ç”¨ç™¾åº¦å¤‡ç”¨æ•°æ®...")
            return [{
                'title': item['title'],
                'url': f"https://www.baidu.com/s?wd={item['title']}",
                'source': 'ç™¾åº¦',
                'platform': 'baidu',
                'score': str(item['hotScore']),
                'type': 'hot'
            } for item in self.MOCK_BAIDU[:limit]]
        
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– ç™¾åº¦çƒ­æœ...")
            resp = self.session.get(
                'https://top.baidu.com/api/board?platform=wise&tab=realtime',
                timeout=10
            )
            data = resp.json()
            
            items = []
            for item in data.get('data', {}).get('cards', [{}])[0].get('content', [])[:limit]:
                title = item.get('word', '')
                if title:
                    items.append({
                        'title': title,
                        'url': item.get('rawUrl', f"https://www.baidu.com/s?wd={title}"),
                        'source': 'ç™¾åº¦',
                        'platform': 'baidu',
                        'score': str(item.get('hotScore', 0)),
                        'type': 'hot'
                    })
            
            print(f"âœ… ç™¾åº¦: {len(items)} æ¡")
            return items
        except Exception as e:
            print(f"âš ï¸ ç™¾åº¦è·å–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®: {e}")
            return [{
                'title': item['title'],
                'url': f"https://www.baidu.com/s?wd={item['title']}",
                'source': 'ç™¾åº¦',
                'platform': 'baidu',
                'score': str(item['hotScore']),
                'type': 'hot'
            } for item in self.MOCK_BAIDU[:limit]]
    
    def fetch_all(self):
        """è·å–æ‰€æœ‰å¹³å°çƒ­æ¦œ"""
        print("\n" + "="*60)
        print("ğŸŒ å¤šå¹³å°çƒ­æ¦œè·å–")
        print("="*60 + "\n")
        
        all_data = {
            'zhihu': self.fetch_zhihu(10),
            'weibo': self.fetch_weibo(10),
            'hackernews': self.fetch_hackernews(10),
            'baidu': self.fetch_baidu(10),
            'updated_at': datetime.now().isoformat()
        }
        
        # ä¿å­˜åŸå§‹æ•°æ®
        Path('api').mkdir(exist_ok=True)
        with open('api/trending_raw.json', 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        total = sum(len(v) for k, v in all_data.items() if isinstance(v, list))
        print(f"\nğŸ“Š æ€»è®¡è·å–: {total} æ¡çƒ­æ¦œæ•°æ®")
        
        return all_data


class AIAnalyzer:
    """AIçƒ­ç‚¹åˆ†æå™¨ - åˆ†æçƒ­ç‚¹å…³è”æ€§å’Œè¶‹åŠ¿"""
    
    # AIç›¸å…³å…³é”®è¯
    AI_KEYWORDS = [
        'AI', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'LLM', 'ChatGPT', 'Claude', 'Gemini',
        'OpenAI', 'DeepSeek', 'GPT', 'æœºå™¨å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'ç®—æ³•',
        'AGI', 'AIGC', 'ç”Ÿæˆå¼AI', 'å¤šæ¨¡æ€', 'Transformer', 'æ·±åº¦å­¦ä¹ ',
        'æ¨ç†æ¨¡å‹', 'æ™ºèƒ½ä½“', 'Agent', 'AIèŠ¯ç‰‡', 'ç®—åŠ›', 'NVIDIA',
        'ç™¾åº¦', 'æ–‡å¿ƒ', 'é€šä¹‰åƒé—®', 'è±†åŒ…', 'Kimi', 'æ™ºè°±', 'æ··å…ƒ',
        'robot', 'æœºå™¨äºº', 'å…·èº«æ™ºèƒ½', 'è®¡ç®—æœºè§†è§‰', 'NLP', 'è‡ªç„¶è¯­è¨€',
        'AIåº”ç”¨', 'AIäº§å“', 'AIå…¬å¸', 'èèµ„', 'æŠ•èµ„', 'startup',
        'Gemini', 'Operator', 'Stargate', 'AI Agent', 'MuMu', 'Player'
    ]
    
    # çƒ­ç‚¹ç±»åˆ«æ˜ å°„
    CATEGORY_MAP = {
        'å¤§æ¨¡å‹': ['GPT', 'Claude', 'Gemini', 'DeepSeek', 'Llama', 'å¤§æ¨¡å‹', 'LLM', 'åŸºç¡€æ¨¡å‹', 'Gemini'],
        'äº§å“å‘å¸ƒ': ['å‘å¸ƒ', 'ä¸Šçº¿', 'æ¨å‡º', 'æ–°å“', 'APP', 'åº”ç”¨', 'Pro', 'å‘å¸ƒ'],
        'æŠ€æœ¯çªç ´': ['çªç ´', 'åˆ›æ–°', 'æ¶æ„', 'ç®—æ³•', 'è®ºæ–‡', 'ç ”ç©¶', 'Consistency', 'Diffusion'],
        'æŠ•èèµ„': ['èèµ„', 'ä¼°å€¼', 'æŠ•èµ„', 'IPO', 'ä¸Šå¸‚', 'ç‹¬è§’å…½', 'Stargate'],
        'äº§ä¸šåŠ¨æ€': ['äº§ä¸š', 'è¡Œä¸š', 'å¸‚åœº', 'ç”Ÿæ€', 'æ”¿ç­–', 'ç›‘ç®¡'],
        'ç¡¬ä»¶èŠ¯ç‰‡': ['èŠ¯ç‰‡', 'GPU', 'NVIDIA', 'ç®—åŠ›', 'æ¨ç†', 'è®­ç»ƒ', 'é›†ç¾¤'],
        'AIåº”ç”¨': ['åº”ç”¨', 'è½åœ°', 'å•†ä¸šåŒ–', 'äº§å“', 'ç”¨æˆ·', 'DAU'],
        'å¼€æºç”Ÿæ€': ['å¼€æº', 'GitHub', 'ç¤¾åŒº', 'å¼€å‘è€…', 'æƒé‡', 'æ¨¡å‹'],
        'AIä¼¦ç†': ['Agent', 'æ™ºèƒ½ä½“', 'å®‰å…¨', 'ä¼¦ç†', 'é£é™©', 'éšç§', 'Reconnaissance'],
    }
    
    def __init__(self, trending_data):
        self.data = trending_data
        self.ai_items = []
        
    def filter_ai_items(self):
        """ç­›é€‰AIç›¸å…³çƒ­ç‚¹"""
        all_items = []
        for platform, items in self.data.items():
            if isinstance(items, list):
                for item in items:
                    item['platform'] = platform
                    all_items.append(item)
        
        # ç­›é€‰AIç›¸å…³
        ai_items = []
        for item in all_items:
            title = item.get('title', '').lower()
            if any(kw.lower() in title for kw in self.AI_KEYWORDS):
                # æ·»åŠ AIç›¸å…³æ€§åˆ†æ•°
                item['ai_score'] = sum(1 for kw in self.AI_KEYWORDS if kw.lower() in title)
                ai_items.append(item)
        
        # æŒ‰AIç›¸å…³æ€§æ’åº
        ai_items.sort(key=lambda x: x.get('ai_score', 0), reverse=True)
        self.ai_items = ai_items[:30]  # å–å‰30æ¡
        
        print(f"\nğŸ¤– AIç›¸å…³çƒ­ç‚¹: {len(self.ai_items)} æ¡")
        return self.ai_items
    
    def categorize_items(self):
        """ä¸ºçƒ­ç‚¹åˆ†ç±»"""
        for item in self.ai_items:
            title = item.get('title', '')
            categories = []
            
            for cat, keywords in self.CATEGORY_MAP.items():
                if any(kw in title for kw in keywords):
                    categories.append(cat)
            
            if not categories:
                categories = ['ç»¼åˆ']
            
            item['categories'] = categories
            item['primary_category'] = categories[0]
        
        return self.ai_items
    
    def analyze_trends(self):
        """åˆ†æçƒ­ç‚¹è¶‹åŠ¿ - è·¨å¹³å°å…±æŒ¯"""
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å¹³å°åˆ†å¸ƒ
        category_platforms = {}
        for item in self.ai_items:
            cat = item.get('primary_category', 'å…¶ä»–')
            platform = item.get('platform', 'unknown')
            
            if cat not in category_platforms:
                category_platforms[cat] = set()
            category_platforms[cat].add(platform)
        
        # æ‰¾å‡ºè·¨å¹³å°çƒ­ç‚¹ï¼ˆåœ¨å¤šä¸ªå¹³å°å‡ºç°ï¼‰
        cross_platform = {}
        for cat, platforms in category_platforms.items():
            if len(platforms) >= 2:
                cross_platform[cat] = list(platforms)
        
        return {
            'cross_platform_topics': cross_platform,
            'category_distribution': {k: len(v) for k, v in category_platforms.items()}
        }
    
    def generate_insight(self):
        """ç”Ÿæˆä»Šæ—¥çƒ­ç‚¹è§£è¯»"""
        if not self.ai_items:
            return "ä»Šæ—¥AIçƒ­ç‚¹è¾ƒå°‘ï¼Œè¯·ç¨åå†è¯•ã€‚"
        
        # è·å–å‰å‡ å¤§ç±»åˆ«çš„çƒ­ç‚¹
        category_items = {}
        for item in self.ai_items[:15]:
            cat = item.get('primary_category', 'ç»¼åˆ')
            if cat not in category_items:
                category_items[cat] = []
            category_items[cat].append(item)
        
        # ç”Ÿæˆè§£è¯»æ–‡æœ¬
        lines = []
        lines.append("## ä»Šæ—¥AIçƒ­ç‚¹æ€åŠ¿\n")
        
        # ä¸»è¦è¶‹åŠ¿
        top_categories = sorted(category_items.keys(), 
                               key=lambda x: len(category_items[x]), 
                               reverse=True)[:3]
        
        lines.append(f"**æ ¸å¿ƒä¸»çº¿**ï¼šä»Šæ—¥AIçƒ­ç‚¹å›´ç»•ã€Œ{'ã€'.join(top_categories)}ã€å±•å¼€ã€‚")
        
        # è·¨å¹³å°å…±æŒ¯
        trends = self.analyze_trends()
        cross = trends.get('cross_platform_topics', {})
        if cross:
            cross_cats = list(cross.keys())[:2]
            lines.append(f"ã€Œ{'ã€'.join(cross_cats)}ã€è¯é¢˜åœ¨å¤šå¹³å°å¼•å‘çƒ­è®®ï¼Œæ˜¾ç¤ºè¡Œä¸šå…±è¯†æ­£åœ¨å½¢æˆã€‚\n")
        
        # åˆ†ç±»è§£è¯»
        lines.append("\n**çƒ­ç‚¹åˆ†å¸ƒ**ï¼š")
        for cat in top_categories[:3]:
            items = category_items[cat][:2]
            titles = [item['title'][:25] + "..." if len(item['title']) > 25 else item['title'] for item in items]
            lines.append(f"- **{cat}**ï¼š{'ã€'.join(titles)}")
        
        # å›½é™…vså›½å†…
        domestic = [i for i in self.ai_items if i.get('platform') in ['zhihu', 'weibo', 'baidu']]
        international = [i for i in self.ai_items if i.get('platform') == 'hackernews']
        
        lines.append(f"\n**èˆ†è®ºé£å‘**ï¼š")
        lines.append(f"- å›½å†…èšç„¦ï¼šäº§å“è½åœ°ä¸å•†ä¸šåº”ç”¨ï¼ˆ{len(domestic)}æ¡ï¼‰")
        lines.append(f"- å›½é™…å…³æ³¨ï¼šæŠ€æœ¯åˆ›æ–°ä¸å¼€æºç”Ÿæ€ï¼ˆ{len(international)}æ¡ï¼‰")
        
        lines.append("\n**ç ”åˆ¤å»ºè®®**ï¼š")
        lines.append("- æŠ•èµ„è€…ï¼šå…³æ³¨æœ‰ä¸šç»©æ”¯æ’‘çš„AIåº”ç”¨æ ‡çš„ï¼Œè­¦æƒ•çº¯æ¦‚å¿µç‚’ä½œ")
        lines.append("- å¼€å‘è€…ï¼šå¤§æ¨¡å‹APIæˆæœ¬æŒç»­ä¸‹é™ï¼Œæ˜¯æ„å»ºAIåº”ç”¨çš„å¥½æ—¶æœº")
        lines.append("- ä»ä¸šè€…ï¼šå¤šæ¨¡æ€å’ŒAI Agentæ˜¯è¿‘æœŸæœ€å€¼å¾—å…³æ³¨çš„æ–¹å‘")
        
        return '\n'.join(lines)
    
    def select_top_news(self, count=20):
        """ç²¾é€‰Topæ–°é—» - å¹³è¡¡å›½å†…å¤–ã€ä¸åŒç±»åˆ«"""
        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_category = {}
        for item in self.ai_items:
            cat = item.get('primary_category', 'ç»¼åˆ')
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(item)
        
        # æŒ‰å¹³å°åˆ†ç»„
        by_platform = {
            'domestic': [i for i in self.ai_items if i.get('platform') in ['zhihu', 'weibo', 'baidu']],
            'international': [i for i in self.ai_items if i.get('platform') == 'hackernews']
        }
        
        selected = []
        
        # ç¡®ä¿å›½å†…å¤–å¹³è¡¡ï¼ˆå„10æ¡ï¼‰
        domestic_count = count // 2
        intl_count = count - domestic_count
        
        # ä»å›½å†…é€‰
        domestic_selected = by_platform['domestic'][:domestic_count]
        for item in domestic_selected:
            item['region'] = 'å›½å†…'
            selected.append(item)
        
        # ä»å›½é™…é€‰
        intl_selected = by_platform['international'][:intl_count]
        for item in intl_selected:
            item['region'] = 'å›½é™…'
            selected.append(item)
        
        # è¡¥å……åˆ°20æ¡ï¼ˆå¦‚æœä¸å¤Ÿï¼‰
        if len(selected) < count:
            remaining = [i for i in self.ai_items if i not in selected]
            for item in remaining[:count - len(selected)]:
                item['region'] = 'å›½å†…' if item.get('platform') != 'hackernews' else 'å›½é™…'
                selected.append(item)
        
        return selected[:count]
    
    def generate_recommended_reading(self):
        """ç”Ÿæˆæ¨èé˜…è¯»"""
        # ä»çŸ¥ä¹å’ŒHNä¸­é€‰å‡ºæœ€æœ‰æ·±åº¦çš„å†…å®¹
        zhihu_depth = [i for i in self.ai_items if i.get('platform') == 'zhihu'][:2]
        hn_depth = [i for i in self.ai_items if i.get('platform') == 'hackernews'][:2]
        
        recommended = []
        for item in zhihu_depth + hn_depth:
            recommended.append({
                'title': item['title'],
                'url': item['url'],
                'source': item['source'],
                'platform': item['platform']
            })
        
        return recommended[:4]


class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨ - ç”Ÿæˆæœ€ç»ˆçš„ç½‘ç«™æ•°æ®"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def generate_summary(self, title, source=''):
        """ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
        # æ ¹æ®æ ‡é¢˜å…³é”®è¯ç”Ÿæˆæ‘˜è¦
        templates = {
            'DeepSeek': 'DeepSeekå‘å¸ƒçš„å¼€æºæ¨¡å‹å¼•å‘è¡Œä¸šéœ‡åŠ¨ã€‚{title}åœ¨æ¨ç†èƒ½åŠ›å’Œä»£ç ç”Ÿæˆæ–¹é¢å®ç°é‡å¤§çªç ´ï¼Œä»¥æä½çš„è®­ç»ƒæˆæœ¬è¾¾åˆ°é¡¶çº§æ€§èƒ½æ°´å¹³ï¼Œä¸ºä¸­å›½AIæŠ€æœ¯å‘å±•æ ‘ç«‹æ–°æ ‡æ†ã€‚',
            'OpenAI': 'OpenAIæŒç»­å¼•é¢†AIè¡Œä¸šå‘å±•ã€‚{title}æ ‡å¿—ç€äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å®ç”¨åŒ–å’Œå•†ä¸šåŒ–æ–¹é¢è¿ˆå‡ºé‡è¦ä¸€æ­¥ï¼Œä¸ºå¼€å‘è€…å’Œä¼ä¸šå¸¦æ¥æ–°çš„å¯èƒ½æ€§ã€‚',
            'Gemini': 'Google Geminiç³»åˆ—æ¨¡å‹å†æ¬¡å‡çº§ã€‚{title}å±•ç°äº†å¤šæ¨¡æ€AIçš„å¼ºå¤§æ½œåŠ›ï¼Œåœ¨ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸Šå®ç°æ˜¾è‘—æå‡ï¼Œä¸GPT-4å±•å¼€æ¿€çƒˆç«äº‰ã€‚',
            'Agent': 'AIæ™ºèƒ½ä½“æŠ€æœ¯è¿›å…¥å®ç”¨åŒ–é˜¶æ®µã€‚{title}å¼•å‘å…³äºAIè‡ªä¸»æ€§å’Œå®‰å…¨æ€§çš„æ·±åº¦è®¨è®ºï¼ŒåŒæ—¶ä¹Ÿå±•ç°äº†æ™ºèƒ½è‡ªåŠ¨åŒ–åœ¨å„è¡Œå„ä¸šçš„åº”ç”¨å‰æ™¯ã€‚',
            'Stargate': 'å·¨é¢æŠ•èµ„å½°æ˜¾AIåŸºç¡€è®¾æ–½æˆ˜ç•¥ä»·å€¼ã€‚{title}ä½“ç°äº†å„å›½å’Œä¼ä¸šå¯¹AIé¢†å¯¼åœ°ä½çš„é‡è§†ï¼Œå°†æ˜¾è‘—åŠ é€ŸAIç®—åŠ›å»ºè®¾å’Œäº§ä¸šå‘å±•ã€‚',
            'æŠ€æœ¯çªç ´': 'AIé¢†åŸŸè¿æ¥æ–°çš„æŠ€æœ¯çªç ´ã€‚{title}æœ‰æœ›è§£å†³å½“å‰å¤§æ¨¡å‹é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•å¥ å®šé‡è¦åŸºç¡€ã€‚',
            'å®‰å…¨': 'AIå®‰å…¨é—®é¢˜å¼•å‘å¹¿æ³›å…³æ³¨ã€‚{title}æé†’æˆ‘ä»¬åœ¨è¿½æ±‚æŠ€æœ¯å‘å±•çš„åŒæ—¶ï¼Œå¿…é¡»é‡è§†éšç§ä¿æŠ¤å’Œä¼¦ç†é£é™©ï¼Œå»ºç«‹å®Œå–„çš„ç›‘ç®¡æœºåˆ¶ã€‚',
            'default': '{title}å¼•å‘è¡Œä¸šçƒ­è®®ã€‚è¿™ä¸€åŠ¨æ€åæ˜ äº†AIæŠ€æœ¯å¿«é€Ÿå‘å±•çš„è¶‹åŠ¿ï¼Œå€¼å¾—æŠ€æœ¯ä»ä¸šè€…ã€æŠ•èµ„è€…å’Œä¼ä¸šå†³ç­–è€…å¯†åˆ‡å…³æ³¨ã€‚'
        }
        
        for keyword, template in templates.items():
            if keyword in title:
                summary = template.format(title=title[:40])
                # ç¡®ä¿200å­—ä»¥ä¸Š
                if len(summary) < 200:
                    summary += " æ®ç›¸å…³åˆ†æï¼Œè¿™ä¸€è¿›å±•å°†å¯¹AIäº§ä¸šé“¾ä¸Šä¸‹æ¸¸äº§ç”Ÿæ·±è¿œå½±å“ï¼Œæ¨åŠ¨æŠ€æœ¯æ™®æƒ å’Œåº”ç”¨åˆ›æ–°ã€‚"
                return summary
        
        summary = templates['default'].format(title=title[:40])
        if len(summary) < 200:
            summary += " æ®ç›¸å…³åˆ†æï¼Œè¿™ä¸€è¿›å±•å°†å¯¹AIäº§ä¸šé“¾ä¸Šä¸‹æ¸¸äº§ç”Ÿæ·±è¿œå½±å“ï¼Œæ¨åŠ¨æŠ€æœ¯æ™®æƒ å’Œåº”ç”¨åˆ›æ–°ã€‚"
        return summary
    
    def generate_api_data(self):
        """ç”Ÿæˆç½‘ç«™APIæ•°æ®"""
        # ç²¾é€‰20æ¡æ–°é—»
        top_news = self.analyzer.select_top_news(20)
        
        # ç”Ÿæˆçƒ­ç‚¹è§£è¯»
        insight = self.analyzer.generate_insight()
        
        # ç”Ÿæˆæ¨èé˜…è¯»
        recommended = self.analyzer.generate_recommended_reading()
        
        # æ„å»ºAPIæ•°æ®
        api_data = {
            "version": "2.0",
            "lastUpdated": datetime.now().isoformat(),
            "insight": {
                "title": "ä»Šæ—¥AIçƒ­ç‚¹è§£è¯»",
                "content": insight,
                "updatedAt": datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
            },
            "categories": [
                {
                    "id": "hot",
                    "name": "AIçƒ­ç‚¹",
                    "articles": []
                }
            ],
            "recommended": recommended
        }
        
        # ç”Ÿæˆæ–‡ç« å¡ç‰‡
        for i, item in enumerate(top_news, 1):
            region = item.get('region', 'å›½å†…')
            category = item.get('primary_category', 'AIçƒ­ç‚¹')
            
            # ç”Ÿæˆæ‘˜è¦
            summary = self.generate_summary(item['title'], item.get('source', ''))
            
            article = {
                "id": f"news_{i:02d}",
                "title": item['title'],
                "summary": summary[:300],
                "tag": f"{region} Â· {category}",
                "source": item.get('source', 'Tech News'),
                "date": "ä»Šå¤©",
                "url": item['url'],
                "views": int(item.get('score', 5000)) * 10 if str(item.get('score')).isdigit() else 5000 + i * 100,
                "isHot": i <= 3,
                "platform": item.get('platform', ''),
                "category": category
            }
            api_data["categories"][0]["articles"].append(article)
        
        return api_data


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ TechInsight Hub - å¤šå¹³å°çƒ­æ¦œèšåˆç³»ç»Ÿ v2.1")
    print("="*60)
    
    # åˆ›å»ºç›®å½•
    Path('api').mkdir(exist_ok=True)
    
    # 1. è·å–å¤šå¹³å°çƒ­æ¦œï¼ˆä½¿ç”¨å¤‡ç”¨æ•°æ®æ¨¡å¼ï¼‰
    fetcher = TrendingFetcher(use_mock=True)
    trending_data = fetcher.fetch_all()
    
    # 2. AIåˆ†æ
    print("\n" + "="*60)
    print("ğŸ¤– AIçƒ­ç‚¹åˆ†æ")
    print("="*60)
    
    analyzer = AIAnalyzer(trending_data)
    analyzer.filter_ai_items()
    analyzer.categorize_items()
    
    # è¾“å‡ºåˆ†æç»“æœ
    trends = analyzer.analyze_trends()
    print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
    for cat, count in trends['category_distribution'].items():
        print(f"   {cat}: {count}æ¡")
    
    cross = trends.get('cross_platform_topics', {})
    if cross:
        print(f"\nğŸ”¥ è·¨å¹³å°çƒ­ç‚¹: {', '.join(cross.keys())}")
    
    # ç”Ÿæˆçƒ­ç‚¹è§£è¯»
    insight = analyzer.generate_insight()
    print("\nğŸ“ ä»Šæ—¥çƒ­ç‚¹è§£è¯»:")
    print("-" * 60)
    print(insight[:800] + "...")
    print("-" * 60)
    
    # 3. ç”Ÿæˆå†…å®¹
    print("\n" + "="*60)
    print("ğŸ“¦ ç”Ÿæˆç½‘ç«™å†…å®¹")
    print("="*60)
    
    generator = ContentGenerator(analyzer)
    api_data = generator.generate_api_data()
    
    # ä¿å­˜æ•°æ®
    with open('api/tech-news.json', 'w', encoding='utf-8') as f:
        json.dump(api_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜çƒ­ç‚¹è§£è¯»å•ç‹¬æ–‡ä»¶
    with open('api/daily_insight.md', 'w', encoding='utf-8') as f:
        f.write(insight)
    
    print(f"\nâœ… ç”Ÿæˆå®Œæˆ!")
    print(f"   ğŸ“° AIçƒ­ç‚¹: {len(api_data['categories'][0]['articles'])} æ¡")
    print(f"   ğŸ“– æ¨èé˜…è¯»: {len(api_data['recommended'])} æ¡")
    print(f"   ğŸ“ çƒ­ç‚¹è§£è¯»: å·²ç”Ÿæˆ")
    print("="*60)


if __name__ == '__main__':
    main()
