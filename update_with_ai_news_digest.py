#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ ai-news-digest æŠ€èƒ½æ–¹å¼èŽ·å–ä¸­æ–‡ AI æ–°é—»å¹¶æ›´æ–°ç½‘ç«™
è‡ªåŠ¨è§£æžçœŸå®žæ–‡ç« é“¾æŽ¥
"""

import json
import os
from datetime import datetime
from duckduckgo_search import DDGS

def resolve_real_url(title, source):
    """é€šè¿‡æœç´¢èŽ·å–çœŸå®žæ–‡ç« URL"""
    try:
        query = f"{title} {source}"
        with DDGS() as ddgs:
            # ä½¿ç”¨æ–‡æœ¬æœç´¢èŽ·å–çœŸå®žé“¾æŽ¥
            results = list(ddgs.text(query, region='cn-zh', max_results=3))
            if results:
                # è¿”å›žç¬¬ä¸€ä¸ªç»“æžœçš„çœŸå®žURL
                return results[0]['href']
    except Exception as e:
        print(f"âš ï¸ è§£æžé“¾æŽ¥å¤±è´¥: {e}")
    
    # å¦‚æžœå¤±è´¥ï¼Œè¿”å›žBingæœç´¢é“¾æŽ¥
    return f"https://www.bing.com/search?q={title.replace(' ', '+')}"

def search_ai_news():
    """æœç´¢ä¸­æ–‡ AI æ–°é—»ï¼ˆai-news-digest æŠ€èƒ½æ–¹å¼ï¼‰"""
    print("ðŸ” ä½¿ç”¨ ai-news-digest æŠ€èƒ½æœç´¢æ–°é—»...")
    
    sources_queries = [
        ('æœºå™¨ä¹‹å¿ƒ AIäººå·¥æ™ºèƒ½', 'æœºå™¨ä¹‹å¿ƒ'),
        ('36æ°ª AIå¤§æ¨¡åž‹', '36æ°ª'),
        ('InfoQ AIæŠ€æœ¯', 'InfoQ'),
        ('é‡å­ä½ AIäººå·¥æ™ºèƒ½', 'é‡å­ä½'),
        ('è…¾è®¯ç§‘æŠ€ AIäººå·¥æ™ºèƒ½', 'è…¾è®¯ç§‘æŠ€'),
        ('æ–°æµªç§‘æŠ€ AIå¤§æ¨¡åž‹', 'æ–°æµªç§‘æŠ€'),
    ]
    
    all_news = []
    
    with DDGS() as ddgs:
        for query, source_name in sources_queries:
            try:
                results = list(ddgs.news(
                    query, 
                    region='cn-zh', 
                    timelimit='d',
                    max_results=5
                ))
                
                for r in results:
                    # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
                    url = r['url']
                    title = r['title']
                    
                    # å¦‚æžœæ˜¯æ ¹åŸŸåæˆ–æ— æ•ˆé“¾æŽ¥ï¼Œå°è¯•è§£æžçœŸå®žé“¾æŽ¥
                    if url in ['https://finance.sina.com.cn', 'https://www.chinaz.com', 
                               'https://www.jiqizhixin.com', 'https://www.guancha.cn', 
                               'https://www.36kr.com', 'https://new.qq.com',
                               'https://www.pingwest.com', 'https://www.sohu.com', 
                               'https://www.sina.com.cn'] or 'bing.com' in url:
                        print(f"ðŸ” è§£æžçœŸå®žé“¾æŽ¥: {title[:30]}...")
                        url = resolve_real_url(title, r.get('source', source_name))
                    
                    news = {
                        'title': title,
                        'source': r.get('source', source_name),
                        'url': url,
                        'date': r['date'][:10] if 'date' in r else datetime.now().strftime('%Y-%m-%d'),
                        'body': r['body']
                    }
                    all_news.append(news)
                    
            except Exception as e:
                print(f"âš ï¸ æœç´¢ {source_name} æ—¶å‡ºé”™: {e}")
                continue
    
    # åŽ»é‡
    seen = set()
    unique_news = []
    for n in all_news:
        if n['title'] not in seen:
            seen.add(n['title'])
            unique_news.append(n)
    
    print(f"âœ… æ‰¾åˆ° {len(unique_news)} æ¡æ–°é—»")
    return unique_news

def deduplicate_news(news_list):
    """åŽ»é‡ï¼ˆai-news-digest æŠ€èƒ½æ–¹å¼ï¼‰"""
    dedup_file = 'skills/ai-news-digest/data/news-sent.txt'
    
    sent_headlines = set()
    if os.path.exists(dedup_file):
        with open(dedup_file, 'r', encoding='utf-8') as f:
            for line in f:
                if '|' in line:
                    headline = line.split('|')[1].strip()
                    sent_headlines.add(headline)
    
    filtered = []
    for news in news_list:
        is_duplicate = False
        for sent in sent_headlines:
            if len(set(news['title']) & set(sent)) / len(set(news['title'])) > 0.5:
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered.append(news)
    
    print(f"ðŸ“ åŽ»é‡åŽå‰©ä½™ {len(filtered)} æ¡æ–°é—»")
    return filtered

def curate_news(news_list):
    """ç²¾é€‰æ–°é—»ï¼ˆai-news-digest æŠ€èƒ½æ–¹å¼ï¼‰"""
    categorized = {
        'breaking': [],
        'business': [],
        'product': [],
        'research': [],
        'other': []
    }
    
    for news in news_list:
        title = news['title']
        
        if any(kw in title for kw in ['é¦–è¶…', 'çªç ´', 'é‡ç£…', 'ç‚¸è£‚', 'éœ¸æ¦œ', 'é‡Œç¨‹ç¢‘', 'åŽ†å²æ€§']):
            categorized['breaking'].append(news)
        elif any(kw in title for kw in ['è´¢æŠ¥', 'æ”¶å…¥', 'èžèµ„', 'IPO', 'æŠ•èµ„', 'æ”¶è´­', 'å•†ä¸š', 'å¸‚åœº']):
            categorized['business'].append(news)
        elif any(kw in title for kw in ['å‘å¸ƒ', 'ä¸Šçº¿', 'æŽ¨å‡º', 'å¼€æº', 'æ–°å“', 'æ¨¡åž‹']):
            categorized['product'].append(news)
        elif any(kw in title for kw in ['ç ”ç©¶', 'è®ºæ–‡', 'æŠ€æœ¯', 'ç®—æ³•', 'çªç ´']):
            categorized['research'].append(news)
        else:
            categorized['other'].append(news)
    
    curated = []
    for cat in ['breaking', 'business', 'product', 'research', 'other']:
        curated.extend(categorized[cat][:4])
        if len(curated) >= 15:
            break
    
    curated = curated[:15]
    
    for news in curated:
        if len(news['body']) < 200:
            news['body'] += 'ã€‚è¿™ä¸€å‘å±•è¶‹åŠ¿åæ˜ äº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨äº§ä¸šåº”ç”¨ä¸­çš„ä¸æ–­æ·±åŒ–ï¼Œé¢„ç¤ºç€æœªæ¥å°†æœ‰æ›´å¤šåˆ›æ–°åº”ç”¨è½åœ°ï¼ŒæŽ¨åŠ¨æ•´ä¸ªè¡Œä¸šå‘æ›´é«˜æ°´å¹³è¿ˆè¿›ã€‚'
    
    print(f"âœ¨ ç²¾é€‰ {len(curated)} æ¡æ–°é—»")
    return curated

def update_dedup_tracker(news_list):
    """æ›´æ–°åŽ»é‡è¿½è¸ªå™¨"""
    dedup_file = 'skills/ai-news-digest/data/news-sent.txt'
    os.makedirs(os.path.dirname(dedup_file), exist_ok=True)
    
    today = datetime.now().strftime('%Y-%m-%d')
    with open(dedup_file, 'a', encoding='utf-8') as f:
        for news in news_list:
            f.write(f"{today}|{news['title']}\n")
    
    print(f"ðŸ“ å·²æ›´æ–°åŽ»é‡è¿½è¸ªå™¨")

def save_news_data(news_list):
    """ä¿å­˜æ–°é—»æ•°æ®"""
    data = {
        'date': datetime.now().strftime('%Y-%m-%d'),
        'count': len(news_list),
        'news': news_list
    }
    
    with open('daily_news_data.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ðŸ’¾ å·²ä¿å­˜æ–°é—»æ•°æ®")

def main():
    print("="*60)
    print("ðŸ¤– AI News Digest - ç½‘ç«™è‡ªåŠ¨æ›´æ–°")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    # 1. æœç´¢æ–°é—»ï¼ˆè‡ªåŠ¨è§£æžçœŸå®žé“¾æŽ¥ï¼‰
    news = search_ai_news()
    
    if not news:
        print("âŒ æœªæ‰¾åˆ°æ–°é—»ï¼Œæ›´æ–°å¤±è´¥")
        return
    
    # 2. åŽ»é‡
    news = deduplicate_news(news)
    
    # 3. ç²¾é€‰
    news = curate_news(news)
    
    # 4. ä¿å­˜æ•°æ®
    save_news_data(news)
    
    # 5. æ›´æ–°åŽ»é‡è¿½è¸ªå™¨
    update_dedup_tracker(news)
    
    # 6. ç”Ÿæˆç½‘ç«™
    print("ðŸŒ ç”Ÿæˆç½‘ç«™...")
    os.system('python3 update_website_from_news.py')
    
    print("âœ… æ›´æ–°å®Œæˆï¼")
    print(f"ðŸŒ è®¿é—®åœ°å€: https://mingmfu.github.io/tech/")

if __name__ == '__main__':
    main()
