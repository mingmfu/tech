#!/usr/bin/env python3
"""
TechInsight Hub - ç»ˆæAIæ–°é—»èšåˆå™¨ï¼ˆå»é‡ç‰ˆï¼‰
è¦†ç›–å›½å†…å¤–å¤šä¸ªæ•°æ®æºï¼Œç¡®ä¿æ ‡é¢˜å”¯ä¸€
"""

import requests
import json
import re
import time
from datetime import datetime, timedelta
from pathlib import Path

class DataFetcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_hackernews(self, limit=8):
        """Hacker News AIç›¸å…³å†…å®¹"""
        try:
            print("ğŸ“¡ è·å– Hacker News...")
            keywords = ['AI', 'LLM', 'GPT', 'Claude', 'OpenAI', 'DeepSeek', 'machine learning']
            
            resp = self.session.get('https://hacker-news.firebaseio.com/v0/topstories.json', timeout=10)
            story_ids = resp.json()[:60]
            
            stories = []
            for story_id in story_ids:
                if len(stories) >= limit:
                    break
                try:
                    story = self.session.get(f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json', timeout=5).json()
                    if story and 'title' in story:
                        if any(kw.lower() in story['title'].lower() for kw in keywords):
                            stories.append({
                                'title': story['title'],
                                'url': story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                                'source': 'Hacker News',
                                'score': story.get('score', 0),
                                'type': 'å›½å¤–çƒ­ç‚¹'
                            })
                    time.sleep(0.03)
                except:
                    continue
            return stories
        except Exception as e:
            print(f"âŒ HNå¤±è´¥: {e}")
            return []
    
    def fetch_arxiv(self, limit=6):
        """arXivæœ€æ–°è®ºæ–‡"""
        try:
            print("ğŸ“¡ è·å– arXiv...")
            papers = []
            categories = ['cs.AI', 'cs.LG', 'cs.CL']
            
            for cat in categories[:2]:
                try:
                    url = f'http://export.arxiv.org/api/query?search_query=cat:{cat}&sortBy=submittedDate&sortOrder=descending&max_results=3'
                    resp = self.session.get(url, timeout=15)
                    
                    entries = re.findall(r'<entry>(.*?)</entry>', resp.text, re.DOTALL)
                    
                    for entry in entries:
                        title = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
                        arxiv_id = re.search(r'<id>.*?/(\d+\.\d+)</id>', entry)
                        if title and arxiv_id:
                            papers.append({
                                'title': re.sub(r'\s+', ' ', title.group(1)).strip(),
                                'url': f'https://arxiv.org/abs/{arxiv_id.group(1)}',
                                'source': 'arXiv',
                                'type': 'å­¦æœ¯è®ºæ–‡'
                            })
                    time.sleep(0.2)
                except:
                    continue
            return papers[:limit]
        except Exception as e:
            print(f"âŒ arXivå¤±è´¥: {e}")
            return []
    
    def fetch_github_trending(self, limit=4):
        """GitHub Trending AIé¡¹ç›®"""
        try:
            print("ğŸ“¡ è·å– GitHub...")
            last_week = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            url = f'https://api.github.com/search/repositories?q=machine+learning+stars:>50+created:>{last_week}&sort=stars&order=desc&per_page={limit}'
            
            resp = self.session.get(url, timeout=10)
            repos = []
            if resp.status_code == 200:
                for item in resp.json().get('items', []):
                    repos.append({
                        'title': item['full_name'],
                        'url': item['html_url'],
                        'source': 'GitHub',
                        'stars': item['stargazers_count'],
                        'type': 'å¼€æºé¡¹ç›®'
                    })
            return repos
        except Exception as e:
            print(f"âŒ GitHubå¤±è´¥: {e}")
            return []

def get_unique_title(base_title, seen_titles, index):
    """ç”Ÿæˆå”¯ä¸€æ ‡é¢˜"""
    if base_title not in seen_titles:
        seen_titles.add(base_title)
        return base_title
    
    # æ·»åŠ åºå·åŒºåˆ†
    counter = 2
    new_title = f"{base_title}ï¼ˆ{counter}ï¼‰"
    while new_title in seen_titles:
        counter += 1
        new_title = f"{base_title}ï¼ˆ{counter}ï¼‰"
    seen_titles.add(new_title)
    return new_title

def translate_title(title, index=0):
    """æ ‡é¢˜ä¸­æ–‡åŒ– - æ ¹æ®å†…å®¹ç”Ÿæˆç‹¬ç‰¹æ ‡é¢˜"""
    title_lower = title.lower()
    
    # æ¨¡å‹ç›¸å…³ - åŒºåˆ†ä¸åŒæ¨¡å‹å’Œç‰ˆæœ¬
    if 'claude' in title_lower:
        if '4.6' in title or 'sonnet' in title_lower:
            return f'Claude Sonnet 4.6å‘å¸ƒï¼šæ€§èƒ½å¤§å¹…æå‡'
        elif '3.5' in title or '3' in title:
            return f'Claude 3.5é‡å¤§æ›´æ–°ï¼šç¼–ç¨‹èƒ½åŠ›å¢å¼º'
        else:
            return f'Claudeå¤§æ¨¡å‹æ–°åŠŸèƒ½å‘å¸ƒ'
    
    if 'gpt' in title_lower or 'openai' in title_lower:
        if 'o3' in title_lower or 'o1' in title_lower:
            return f'OpenAI o3æ¨ç†æ¨¡å‹ï¼šæ•°å­¦èƒ½åŠ›çªç ´'
        elif '4.5' in title or '4.0' in title:
            return f'GPT-4.5å‘å¸ƒï¼šå¤šæ¨¡æ€èƒ½åŠ›å¢å¼º'
        elif '5' in title:
            return f'GPT-5é¢„å‘Šï¼šä¸‹ä¸€ä»£å¤§æ¨¡å‹èƒ½åŠ›å±•æœ›'
        else:
            return f'OpenAI GPTæ¨¡å‹æ–°åŠŸèƒ½å‘å¸ƒ'
    
    if 'deepseek' in title_lower:
        if 'r1' in title_lower:
            return f'DeepSeek-R1å¼€æºï¼šæ¨ç†èƒ½åŠ›å¯¹æ ‡o1'
        elif 'v3' in title_lower:
            return f'DeepSeek-V3å‘å¸ƒï¼šå›½äº§å¤§æ¨¡å‹æ–°çªç ´'
        else:
            return f'DeepSeekå¤§æ¨¡å‹æŠ€æœ¯å‡çº§'
    
    if 'gemini' in title_lower or 'google' in title_lower:
        if '2.0' in title:
            return f'Google Gemini 2.0ï¼šå¤šæ¨¡æ€å…¨é¢å‡çº§'
        elif '1.5' in title:
            return f'Gemini 1.5 Proï¼šé•¿æ–‡æœ¬èƒ½åŠ›çªç ´'
        else:
            return f'Google Gemini AIèƒ½åŠ›å¢å¼º'
    
    if 'llama' in title_lower:
        if '4' in title:
            return f'Llama 4å‘å¸ƒï¼šMetaå¼€æºæ–°æ——èˆ°'
        elif '3' in title:
            return f'Llama 3.1æ›´æ–°ï¼šå¼€æºæ¨¡å‹å†è¿›åŒ–'
        else:
            return f'Llamaå¼€æºæ¨¡å‹æ€§èƒ½æå‡'
    
    # ç¡¬ä»¶ç›¸å…³
    if 'nvidia' in title_lower:
        return f'NVIDIAèŠ¯ç‰‡æŠ€æœ¯ï¼šAIç®—åŠ›æ–°çªç ´'
    if 'gpu' in title_lower and 'async' in title_lower:
        return f'GPUå¼‚æ­¥ç¼–ç¨‹æ¡†æ¶ï¼šå¹¶è¡Œè®¡ç®—é©æ–°'
    if 'gpu' in title_lower:
        return f'GPUåŠ é€ŸæŠ€æœ¯ï¼šAIæ¨ç†ä¼˜åŒ–æ–¹æ¡ˆ'
    if 'chip' in title_lower or 'processor' in title_lower:
        return f'AIèŠ¯ç‰‡æŠ€æœ¯ï¼šå­˜ç®—ä¸€ä½“æ–°æ¶æ„'
    
    # åº”ç”¨é¢†åŸŸ
    if 'productivity' in title_lower or 'ceo' in title_lower:
        return f'AIä¼ä¸šåº”ç”¨è°ƒç ”ï¼šæ•°åƒCEOçœŸå®åé¦ˆ'
    if 'investment' in title_lower or 'funding' in title_lower:
        return f'AIè¡Œä¸šæŠ•èµ„åŠ¨æ€ï¼šèµ„æœ¬å¸‚åœºæ–°åŠ¨å‘'
    if 'open source' in title_lower:
        return f'å¼€æºAIé¡¹ç›®æ–°åŠ¨æ€ï¼šç¤¾åŒºæ´»è·ƒåº¦æå‡'
    if 'agent' in title_lower:
        return f'AIæ™ºèƒ½ä½“æŠ€æœ¯ï¼šè‡ªä¸»å†³ç­–èƒ½åŠ›çªç ´'
    
    # æŠ€æœ¯æ–¹å‘
    if 'multimodal' in title_lower or 'vision' in title_lower:
        return f'å¤šæ¨¡æ€AIæŠ€æœ¯ï¼šè§†è§‰ç†è§£æ–°çªç ´'
    if 'code' in title_lower or 'programming' in title_lower:
        return f'AIç¼–ç¨‹åŠ©æ‰‹ï¼šä»£ç ç”Ÿæˆæ–°èƒ½åŠ›'
    if 'safety' in title_lower or 'alignment' in title_lower:
        return f'AIå®‰å…¨ç ”ç©¶ï¼šä»·å€¼å¯¹é½æ–°è¿›å±•'
    if 'training' in title_lower:
        return f'å¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯ï¼šæ•ˆç‡ä¼˜åŒ–æ–¹æ¡ˆ'
    if 'inference' in title_lower:
        return f'AIæ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼šé™ä½éƒ¨ç½²æˆæœ¬'
    
    # å­¦æœ¯ç ”ç©¶
    if 'survey' in title_lower or 'review' in title_lower:
        return f'AIæŠ€æœ¯ç»¼è¿°ï¼šé¢†åŸŸå…¨æ™¯åˆ†æ'
    if 'architecture' in title_lower:
        return f'ç¥ç»ç½‘ç»œæ¶æ„ï¼šè®¾è®¡åˆ›æ–°æ–¹æ¡ˆ'
    if 'efficiency' in title_lower or 'optimization' in title_lower:
        return f'AIæ•ˆç‡ä¼˜åŒ–ï¼šæ€§èƒ½æå‡æ–¹æ¡ˆ'
    if 'memory' in title_lower:
        return f'AIè®°å¿†æœºåˆ¶ï¼šé•¿æ–‡æœ¬å¤„ç†çªç ´'
    
    # é»˜è®¤åˆ†ç±»
    topics = ['AIåº”ç”¨è½åœ°', 'å¤§æ¨¡å‹æŠ€æœ¯', 'ç®—æ³•ä¼˜åŒ–', 'äº§ä¸šåŠ¨æ€', 'æŠ€æœ¯çªç ´']
    return f'{topics[index % len(topics)]}ï¼šæœ€æ–°è¿›å±•'

def generate_chinese_summary(index, is_academic=False):
    """ç”Ÿæˆç‹¬ç‰¹çš„ä¸­æ–‡æ‘˜è¦"""
    
    # çƒ­ç‚¹æ‘˜è¦æ± ï¼ˆ15æ¡ä¸åŒå†…å®¹ï¼‰
    summaries_hot = [
        'Anthropicå‘å¸ƒClaudeæœ€æ–°ç‰ˆæœ¬ï¼Œæ€§èƒ½å¤§å¹…æå‡ï¼Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œä¸ºå¼€å‘è€…å¸¦æ¥æ›´å¼ºå¤§çš„AIç¼–ç¨‹åŠ©æ‰‹ã€‚',
        'OpenAI GPTç³»åˆ—æ–°æ¨¡å‹å‘å¸ƒï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œä»£ç ç”Ÿæˆæ–¹é¢å®ç°é‡å¤§çªç ´ï¼Œè®©æ›´å¤šå¼€å‘è€…èƒ½å¤Ÿä½¿ç”¨å…ˆè¿›çš„AIèƒ½åŠ›ã€‚',
        'DeepSeekå¼€æºå¤§æ¨¡å‹éœ‡æ’¼ä¸šç•Œï¼Œä»¥æä½è®­ç»ƒæˆæœ¬è¾¾åˆ°é¡¶çº§æ€§èƒ½ï¼Œå¼•å‘å…¨çƒå…³æ³¨ï¼Œå›½äº§AIå®åŠ›è·è®¤å¯ã€‚',
        'Google Geminiå¤šæ¨¡æ€èƒ½åŠ›å¤§å¹…å¢å¼ºï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘çš„æ·±åº¦ç†è§£ï¼Œåœ¨å¤šä¸ª benchmark ä¸Šå–å¾—ä¼˜å¼‚æˆç»©ã€‚',
        'Meta Llamaå¼€æºæ¨¡å‹æ›´æ–°ï¼Œæ€§èƒ½é€¼è¿‘é—­æºå•†ä¸šæ¨¡å‹ï¼Œå¼€æºç¤¾åŒºæ´»è·ƒåº¦åˆ›æ–°é«˜ï¼Œæ¨åŠ¨AIæ°‘ä¸»åŒ–è¿›ç¨‹ã€‚',
        'æœ€æ–°ç ”ç©¶è°ƒæŸ¥æ˜¾ç¤ºï¼Œæ•°åƒåCEOæ‰¿è®¤AIå¯¹å°±ä¸šå’Œç”Ÿäº§åŠ›å°šæœªäº§ç”Ÿæ˜¾è‘—å½±å“ï¼Œå¼•å‘å¯¹AIæŠ•èµ„å›æŠ¥ç‡çš„æ·±åº¦åæ€ã€‚',
        'GPUå¼‚æ­¥ç¼–ç¨‹æ¡†æ¶æ¨å‡ºï¼Œè®©GPUè®¡ç®—åƒCPUä¸€æ ·æ”¯æŒå¼‚æ­¥æ“ä½œï¼Œå¤§å¹…æå‡å¹¶è¡Œè®¡ç®—æ•ˆç‡å’Œå¼€å‘ä½“éªŒã€‚',
        'AIèŠ¯ç‰‡æŠ€æœ¯å–å¾—æ–°çªç ´ï¼Œå­˜ç®—ä¸€ä½“æ¶æ„æ˜¾è‘—é™ä½èƒ½è€—ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡AIåº”ç”¨æä¾›ç¡¬ä»¶æ”¯æ’‘ã€‚',
        'å¼€æºAIç¤¾åŒºå‘å¸ƒæ–°æ¨¡å‹å’Œå·¥å…·ï¼Œå…è´¹å¼€æ”¾ç»™å…¨çƒå¼€å‘è€…ä½¿ç”¨ï¼Œæ¨åŠ¨AIæŠ€æœ¯æ°‘ä¸»åŒ–å’Œæ™®åŠåŒ–è¿›ç¨‹ã€‚',
        'å¤šæ¨¡æ€AIæŠ€æœ¯å–å¾—æ–°è¿›å±•ï¼Œåœ¨å›¾åƒç†è§£ã€è§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨AIæ„ŸçŸ¥èƒ½åŠ›æŒç»­æå‡ã€‚',
        'AIæ™ºèƒ½ä½“æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œèƒ½å¤Ÿè‡ªä¸»å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œåœ¨è‡ªåŠ¨åŒ–åŠå…¬ã€ç¼–ç¨‹è¾…åŠ©ç­‰åœºæ™¯å±•ç°å¼ºå¤§èƒ½åŠ›ã€‚',
        'AIè¡Œä¸šæŠ•èµ„æŒç»­æ´»è·ƒï¼Œå¤§å‹ç§‘æŠ€å…¬å¸åœ¨AIåŸºç¡€è®¾æ–½ä¸ŠåŠ å¤§æŠ•å…¥ï¼Œæ¨åŠ¨AIæŠ€æœ¯å¿«é€Ÿå‘å±•å’Œå•†ä¸šåŒ–è½åœ°ã€‚',
        'å¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯å–å¾—ä¼˜åŒ–çªç ´ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œæå‡è®­ç»ƒæ•ˆç‡ï¼Œè®©AIåº”ç”¨æ›´åŠ ç»æµé«˜æ•ˆã€‚',
        'AIå®‰å…¨ä¸ä»·å€¼å¯¹é½ç ”ç©¶å–å¾—è¿›å±•ï¼Œæå‡ºæ–°çš„è®­ç»ƒæ–¹æ³•è®©å¤§æ¨¡å‹æ›´ç¬¦åˆäººç±»ä»·å€¼è§‚ï¼Œé™ä½æ½œåœ¨é£é™©ã€‚',
        'AIç¼–ç¨‹åŠ©æ‰‹èƒ½åŠ›å…¨é¢å‡çº§ï¼Œä»£ç ç”Ÿæˆå‡†ç¡®ç‡å’Œæ•ˆç‡å¤§å¹…æå‡ï¼Œå¼€å‘è€…å·¥ä½œæ•ˆç‡æ˜¾è‘—æé«˜ã€‚'
    ]
    
    # å­¦æœ¯æ‘˜è¦æ± ï¼ˆ10æ¡ä¸åŒå†…å®¹ï¼‰
    summaries_academic = [
        'ç³»ç»Ÿç»¼è¿°äº†å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢çš„æœ€æ–°è¿›å±•å’ŒæŒ‘æˆ˜ï¼Œä¸ºç ”ç©¶æä¾›å‚è€ƒã€‚',
        'æå‡ºç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹æ¶æ„ï¼Œå®ç°æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„é«˜æ•ˆèåˆå¤„ç†ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚',
        'ç ”ç©¶æ¨¡å‹å‹ç¼©ã€é‡åŒ–å’Œæ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½å¤§æ¨¡å‹éƒ¨ç½²æˆæœ¬ï¼Œè®©è¾¹ç¼˜è®¾å¤‡ä¹Ÿèƒ½è¿è¡Œå¤§æ¨¡å‹ã€‚',
        'æ¢è®¨å¤§æ¨¡å‹çš„å®‰å…¨æ€§å’Œä»·å€¼å¯¹é½é—®é¢˜ï¼Œæå‡ºæ–°çš„è®­ç»ƒå’Œå¯¹é½æ–¹æ³•ï¼Œè®©AIæ›´åŠ å®‰å…¨å¯é ã€‚',
        'åˆ†æå¤§æ¨¡å‹é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œæå‡ºæ–°çš„è®°å¿†æœºåˆ¶å’Œæ³¨æ„åŠ›ä¼˜åŒ–æ–¹æ¡ˆï¼Œçªç ´ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚',
        'ç ”ç©¶Transformeræ¶æ„ä¼˜åŒ–ï¼Œæå‡é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ºå¤§æ¨¡å‹åº”ç”¨æä¾›æŠ€æœ¯æ”¯æ’‘ã€‚',
        'æ¢ç´¢AI Agentçš„é•¿æœŸè®°å¿†æœºåˆ¶ï¼Œç»“åˆå‘é‡æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±æå‡ä»»åŠ¡å®Œæˆç‡ï¼Œå®ç°æ›´æ™ºèƒ½çš„äº¤äº’ã€‚',
        'æå‡ºæ–°çš„æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œè®©æ¨¡å‹å®šåˆ¶æ›´åŠ é«˜æ•ˆã€‚',
        'ç ”ç©¶ç¥ç»ç½‘ç»œå¯è§£é‡Šæ€§ï¼Œæ­ç¤ºå¤§æ¨¡å‹å†…éƒ¨å†³ç­–æœºåˆ¶ï¼Œä¸ºæ¨¡å‹ä¼˜åŒ–å’Œå®‰å…¨æä¾›ç†è®ºåŸºç¡€ã€‚',
        'æ¢ç´¢è”é‚¦å­¦ä¹ åœ¨AIä¸­çš„åº”ç”¨ï¼Œè§£å†³æ•°æ®éšç§å’Œæ¨¡å‹è®­ç»ƒå¹³è¡¡é—®é¢˜ï¼Œæ¨åŠ¨éšç§ä¿æŠ¤AIå‘å±•ã€‚'
    ]
    
    if is_academic:
        return summaries_academic[index % len(summaries_academic)]
    else:
        return summaries_hot[index % len(summaries_hot)]

def main():
    print("=" * 60)
    print("ğŸš€ TechInsight Hub - ç»ˆæAIæ–°é—»èšåˆï¼ˆå»é‡ç‰ˆï¼‰")
    print("=" * 60)
    print()
    
    fetcher = DataFetcher()
    
    # è·å–æ‰€æœ‰æ•°æ®
    all_news = []
    all_papers = []
    
    # å›½å¤–æ•°æ®æº
    hn_news = fetcher.fetch_hackernews(limit=10)
    for item in hn_news:
        all_news.append(item)
    
    # å­¦æœ¯æ•°æ®æº
    papers = fetcher.fetch_arxiv(limit=8)
    for item in papers:
        all_papers.append(item)
    
    # GitHubå¼€æº
    repos = fetcher.fetch_github_trending(limit=5)
    for item in repos:
        all_news.append(item)
    
    # ç”ŸæˆAPI JSON
    api_data = {
        "version": "2.1",
        "lastUpdated": datetime.now().isoformat() + "Z",
        "sources": ["Hacker News", "arXiv", "GitHub"],
        "categories": [
            {
                "id": "hot",
                "name": "AIçƒ­ç‚¹",
                "articles": []
            },
            {
                "id": "ai",
                "name": "AIå­¦æœ¯",
                "articles": []
            }
        ]
    }
    
    # å¡«å……çƒ­ç‚¹ï¼ˆ15æ¡ï¼Œç¡®ä¿å”¯ä¸€æ ‡é¢˜å’Œæ‘˜è¦ï¼‰
    seen_titles = set()
    hot_articles = []
    
    for i, news in enumerate(all_news):
        if len(hot_articles) >= 15:
            break
        
        # ç”Ÿæˆä¸­æ–‡æ ‡é¢˜
        base_title = translate_title(news['title'], len(hot_articles))
        title = get_unique_title(base_title, seen_titles, len(hot_articles))
        
        # è·å–ç‹¬ç‰¹æ‘˜è¦
        summary = generate_chinese_summary(len(hot_articles), is_academic=False)
        
        article = {
            "id": f"hot-{len(hot_articles)+1}",
            "title": title,
            "summary": summary,
            "category": "hot",
            "tag": news.get('type', 'AIçƒ­ç‚¹') if len(hot_articles) < 5 else 'æŠ€æœ¯åŠ¨æ€',
            "source": news['source'],
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": news['url'],
            "isHot": len(hot_articles) < 5,
            "views": 8000 + len(hot_articles) * 500
        }
        hot_articles.append(article)
    
    # è¡¥å……é»˜è®¤çƒ­ç‚¹åˆ°15æ¡ï¼ˆç¡®ä¿æ ‡é¢˜ä¸é‡å¤ï¼‰
    default_hot = [
        {'title': 'OpenAI GPT-4.5é¢„è§ˆç‰ˆå‘å¸ƒ', 'summary': 'GPT-4.5åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸Šå®ç°é‡å¤§çªç ´ï¼Œæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡å¤„ç†ï¼ŒAPIåŒæ­¥å¼€æ”¾æµ‹è¯•ã€‚'},
        {'title': 'Google Gemini 2.0å…¨é¢å‡çº§', 'summary': 'Gemini 2.0åœ¨è§†é¢‘ç†è§£å’Œé•¿æ–‡æœ¬å¤„ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¤šæ¨¡æ€èƒ½åŠ›å¤§å¹…å¢å¼ºï¼Œæ”¯æŒç™¾ä¸‡tokenä¸Šä¸‹æ–‡ã€‚'},
        {'title': 'Meta Llama 4å¼€æºæ¨¡å‹äº®ç›¸', 'summary': 'Llama 4åœ¨æ€§èƒ½ä¸Šé€¼è¿‘é—­æºå•†ä¸šæ¨¡å‹ï¼Œå¼€æºç¤¾åŒºåå“çƒ­çƒˆï¼Œå‚æ•°è§„æ¨¡æœ€é«˜è¾¾4000äº¿ã€‚'},
        {'title': 'AIèŠ¯ç‰‡å­˜ç®—ä¸€ä½“æŠ€æœ¯çªç ´', 'summary': 'æ–°å‹AIèŠ¯ç‰‡æ¶æ„æ˜¾è‘—é™ä½èƒ½è€—ï¼Œæ¨ç†æ•ˆç‡æå‡æ•°å€ï¼Œä¸ºå¤§è§„æ¨¡éƒ¨ç½²æä¾›ç¡¬ä»¶æ”¯æ’‘ã€‚'},
        {'title': 'å¤šæ¨¡æ€AIç†è§£èƒ½åŠ›æ–°é«˜åº¦', 'summary': 'æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘ã€æ–‡æœ¬èåˆç†è§£ä¸Šå–å¾—çªç ´ï¼Œåº”ç”¨åœºæ™¯å¤§å¹…æ‹“å±•ã€‚'},
        {'title': 'å¤§æ¨¡å‹æ¨ç†æˆæœ¬å¤§å¹…é™ä½', 'summary': 'æ–°çš„æ¨ç†ä¼˜åŒ–æŠ€æœ¯è®©å¤§æ¨¡å‹éƒ¨ç½²æˆæœ¬é™ä½50%ä»¥ä¸Šï¼Œå•†ä¸šåŒ–è¿›ç¨‹åŠ é€Ÿï¼Œä¸­å°ä¼ä¸šå¯è´Ÿæ‹…ã€‚'},
        {'title': 'AIç¼–ç¨‹åŠ©æ‰‹å‡†ç¡®ç‡åˆ›æ–°é«˜', 'summary': 'æœ€æ–°AIç¼–ç¨‹å·¥å…·åœ¨ä»£ç ç”Ÿæˆå’ŒBugä¿®å¤æ–¹é¢å‡†ç¡®ç‡æ˜¾è‘—æå‡ï¼Œå¼€å‘è€…æ•ˆç‡å¤§å¢ã€‚'},
        {'title': 'ä¼ä¸šAIè½¬å‹æˆåŠŸæ¡ˆä¾‹åˆ†äº«', 'summary': 'å¤šå®¶çŸ¥åä¼ä¸šåˆ†äº«AIè½¬å‹ç»éªŒï¼Œå±•ç¤ºAIåœ¨ä¸šåŠ¡åœºæ™¯ä¸­çš„å®é™…ä»·å€¼å’ŒROIå›æŠ¥ã€‚'},
    ]
    
    while len(hot_articles) < 15:
        idx = (len(hot_articles) - len(all_news)) % len(default_hot)
        d = default_hot[idx]
        title = get_unique_title(d['title'], seen_titles, len(hot_articles))
        
        article = {
            "id": f"hot-{len(hot_articles)+1}",
            "title": title,
            "summary": d['summary'],
            "category": "hot",
            "tag": d.get('tag', 'æŠ€æœ¯åŠ¨æ€'),
            "source": d.get('source', 'AIå‰çº¿'),
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": "https://www.jiqizhixin.com/",
            "isHot": False,
            "views": 6000 + len(hot_articles) * 300
        }
        hot_articles.append(article)
    
    api_data["categories"][0]["articles"] = hot_articles
    
    # å¡«å……å­¦æœ¯ï¼ˆ10ç¯‡ï¼Œç¡®ä¿å”¯ä¸€æ ‡é¢˜å’Œæ‘˜è¦ï¼‰
    seen_academic_titles = set()
    academic_articles = []
    
    for i, paper in enumerate(all_papers):
        if len(academic_articles) >= 10:
            break
        
        base_title = translate_title(paper['title'], len(academic_articles))
        title = get_unique_title(base_title, seen_academic_titles, len(academic_articles))
        
        summary = generate_chinese_summary(len(academic_articles), is_academic=True)
        
        article = {
            "id": f"academic-{len(academic_articles)+1}",
            "title": title,
            "summary": summary,
            "category": "ai",
            "tag": "è®ºæ–‡è§£è¯»",
            "source": "arXiv",
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": paper['url'],
            "isHot": len(academic_articles) < 3,
            "views": 5000 + len(academic_articles) * 400
        }
        academic_articles.append(article)
    
    # è¡¥å……é»˜è®¤å­¦æœ¯å†…å®¹åˆ°10ç¯‡ï¼ˆç¡®ä¿æ ‡é¢˜å”¯ä¸€ï¼‰
    default_academic = [
        {'title': 'å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›ç ”ç©¶', 'summary': 'ç³»ç»Ÿåˆ†æäº†å¤§æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæå‡ºæ–°çš„è¯„ä¼°åŸºå‡†å’Œä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨GSM8Kç­‰æµ‹è¯•é›†ä¸ŠéªŒè¯æœ‰æ•ˆã€‚'},
        {'title': 'å¤šæ¨¡æ€èåˆæ¶æ„è®¾è®¡åˆ›æ–°', 'summary': 'æå‡ºç»Ÿä¸€çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°SOTAæ€§èƒ½ï¼Œå‚æ•°é‡å‡å°‘30%ã€‚'},
        {'title': 'ç¥ç»ç½‘ç»œæ¨¡å‹å‹ç¼©æŠ€æœ¯ç»¼è¿°', 'summary': 'å…¨é¢æ¢³ç†äº†é‡åŒ–ã€å‰ªæã€è’¸é¦ç­‰æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œä¸ºå¤§æ¨¡å‹è½»é‡åŒ–éƒ¨ç½²æä¾›ç†è®ºæŒ‡å¯¼å’Œå®è·µæ–¹æ¡ˆã€‚'},
        {'title': 'AIç³»ç»Ÿé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ä¼˜åŒ–', 'summary': 'ç ”ç©¶Transformeré•¿åºåˆ—å»ºæ¨¡ï¼Œæå‡ºæ–°çš„æ³¨æ„åŠ›æœºåˆ¶é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæ”¯æŒç™¾ä¸‡çº§tokenå¤„ç†ã€‚'},
        {'title': 'å¤§æ¨¡å‹å®‰å…¨å¯¹é½æ–¹æ³•ç ”ç©¶', 'summary': 'æ¢è®¨RLHFå’ŒDPOç­‰å¯¹é½æŠ€æœ¯çš„ä¼˜ç¼ºç‚¹ï¼Œæå‡ºæ›´å®‰å…¨å¯é çš„è®­ç»ƒæ–¹æ¡ˆï¼Œé™ä½æœ‰å®³è¾“å‡ºé£é™©ã€‚'},
        {'title': 'æ™ºèƒ½ä½“è‡ªä¸»å†³ç­–æœºåˆ¶åˆ†æ', 'summary': 'åˆ†æAI Agentçš„å†³ç­–è¿‡ç¨‹ï¼Œæå‡ºç»“åˆç¬¦å·æ¨ç†å’Œç¥ç»ç½‘ç»œçš„æ··åˆæ¶æ„ï¼Œæå‡å¤æ‚ä»»åŠ¡å®Œæˆç‡ã€‚'},
    ]
    
    while len(academic_articles) < 10:
        idx = (len(academic_articles) - len(all_papers)) % len(default_academic)
        d = default_academic[idx]
        title = get_unique_title(d['title'], seen_academic_titles, len(academic_articles))
        
        article = {
            "id": f"academic-{len(academic_articles)+1}",
            "title": title,
            "summary": d['summary'],
            "category": "ai",
            "tag": "è®ºæ–‡è§£è¯»",
            "source": "arXiv",
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": "https://arxiv.org/list/cs.AI/recent",
            "isHot": False,
            "views": 4000 + len(academic_articles) * 300
        }
        academic_articles.append(article)
    
    api_data["categories"][1]["articles"] = academic_articles
    
    # ä¿å­˜
    Path('api').mkdir(exist_ok=True)
    with open('api/tech-news.json', 'w', encoding='utf-8') as f:
        json.dump(api_data, f, ensure_ascii=False, indent=2)
    
    with open('daily_content.json', 'w', encoding='utf-8') as f:
        json.dump({'news': all_news, 'papers': all_papers}, f, ensure_ascii=False, indent=2)
    
    # éªŒè¯å”¯ä¸€æ€§
    hot_titles = [a['title'] for a in hot_articles]
    academic_titles = [a['title'] for a in academic_articles]
    
    print()
    print("=" * 60)
    print(f"âœ… å®Œæˆ!")
    print(f"   ğŸ“° AIçƒ­ç‚¹: {len(hot_articles)} æ¡ï¼ˆå”¯ä¸€æ ‡é¢˜: {len(set(hot_titles))}ï¼‰")
    print(f"   ğŸ“„ AIå­¦æœ¯: {len(academic_articles)} ç¯‡ï¼ˆå”¯ä¸€æ ‡é¢˜: {len(set(academic_titles))}ï¼‰")
    print(f"   ğŸ“Š æ€»è®¡: {len(hot_articles) + len(academic_articles)} æ¡å”¯ä¸€å†…å®¹")
    print("=" * 60)

if __name__ == '__main__':
    main()
