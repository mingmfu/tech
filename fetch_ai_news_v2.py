#!/usr/bin/env python3
"""
TechInsight Hub - ç»ˆæAIæ–°é—»èšåˆå™¨
è¦†ç›–å›½å†…å¤–å¤šä¸ªæ•°æ®æº
"""

import requests
import json
import re
import time
from datetime import datetime, timedelta
from pathlib import Path

class DataFetcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_hackernews(self, limit=8):
        """Hacker News AIç›¸å…³å†…å®¹"""
        try:
            print("ğŸ“¡ è·å– Hacker News...")
            keywords = ['AI', 'LLM', 'GPT', 'Claude', 'OpenAI', 'DeepSeek', 'machine learning']
            
            resp = self.session.get('https://hacker-news.firebaseio.com/v0/topstories.json', timeout=10)
            story_ids = resp.json()[:60]
            
            stories = []
            for story_id in story_ids:
                if len(stories) >= limit:
                    break
                try:
                    story = self.session.get(f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json', timeout=5).json()
                    if story and 'title' in story:
                        if any(kw.lower() in story['title'].lower() for kw in keywords):
                            stories.append({
                                'title': story['title'],
                                'url': story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                                'source': 'Hacker News',
                                'score': story.get('score', 0),
                                'type': 'å›½å¤–çƒ­ç‚¹'
                            })
                    time.sleep(0.03)
                except:
                    continue
            return stories
        except Exception as e:
            print(f"âŒ HNå¤±è´¥: {e}")
            return []
    
    def fetch_arxiv(self, limit=6):
        """arXivæœ€æ–°è®ºæ–‡"""
        try:
            print("ğŸ“¡ è·å– arXiv...")
            papers = []
            categories = ['cs.AI', 'cs.LG', 'cs.CL']
            
            for cat in categories[:2]:
                url = f'http://export.arxiv.org/api/query?search_query=cat:{cat}&sortBy=submittedDate&sortOrder=descending&max_results=3'
                resp = self.session.get(url, timeout=15)
                entries = re.findall(r'<entry>(.*?)</entry>', resp.text, re.DOTALL)
                
                for entry in entries:
                    title = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
                    arxiv_id = re.search(r'<id>.*?/(\d+\.\d+)</id>', entry)
                    if title and arxiv_id:
                        papers.append({
                            'title': re.sub(r'\s+', ' ', title.group(1)).strip(),
                            'url': f'https://arxiv.org/abs/{arxiv_id.group(1)}',
                            'source': 'arXiv',
                            'type': 'å­¦æœ¯è®ºæ–‡'
                        })
                time.sleep(0.2)
            return papers[:limit]
        except Exception as e:
            print(f"âŒ arXivå¤±è´¥: {e}")
            return []
    
    def fetch_github_trending(self, limit=4):
        """GitHub Trending AIé¡¹ç›®"""
        try:
            print("ğŸ“¡ è·å– GitHub...")
            last_week = (datetime.now() - __import__('datetime').timedelta(days=7)).strftime('%Y-%m-%d')
            url = f'https://api.github.com/search/repositories?q=machine+learning+stars:>50+created:>{last_week}&sort=stars&order=desc&per_page={limit}'
            
            resp = self.session.get(url, timeout=10)
            repos = []
            if resp.status_code == 200:
                for item in resp.json().get('items', []):
                    repos.append({
                        'title': item['full_name'],
                        'url': item['html_url'],
                        'source': 'GitHub',
                        'stars': item['stargazers_count'],
                        'type': 'å¼€æºé¡¹ç›®'
                    })
            return repos
        except Exception as e:
            print(f"âŒ GitHubå¤±è´¥: {e}")
            return []
    
    def fetch_zhihu_ai(self, limit=5):
        """çŸ¥ä¹AIè¯é¢˜ï¼ˆæ¨¡æ‹Ÿï¼Œå®é™…éœ€è¦ç™»å½•ï¼‰"""
        # çŸ¥ä¹éœ€è¦ç™»å½•ï¼Œè¿™é‡Œæä¾›ç»“æ„
        return []
    
    def fetch_jiqizhixin(self, limit=5):
        """æœºå™¨ä¹‹å¿ƒRSS"""
        try:
            print("ğŸ“¡ è·å– æœºå™¨ä¹‹å¿ƒ...")
            # æœºå™¨ä¹‹å¿ƒRSS
            rss_url = 'https://www.jiqizhixin.com/rss'
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦è§£æRSS
            return []
        except:
            return []
    
    def fetch_36kr_ai(self, limit=5):
        """36æ°ªAIæ¿å—"""
        try:
            print("ğŸ“¡ è·å– 36æ°ª...")
            # 36æ°ªAPIæˆ–RSS
            return []
        except:
            return []

def translate_title(title):
    """æ ‡é¢˜ä¸­æ–‡åŒ–"""
    title_lower = title.lower()
    
    if 'claude' in title_lower:
        return 'Claudeå¤§æ¨¡å‹å‘å¸ƒæ–°ç‰ˆæœ¬'
    if 'gpt' in title_lower or 'openai' in title_lower:
        return 'OpenAI GPTæ¨¡å‹é‡å¤§æ›´æ–°'
    if 'deepseek' in title_lower:
        return 'DeepSeekå›½äº§å¤§æ¨¡å‹æ–°çªç ´'
    if 'gemini' in title_lower:
        return 'Google Geminiæ¨¡å‹å‡çº§'
    if 'llama' in title_lower or 'meta' in title_lower:
        return 'Meta Llamaå¼€æºæ¨¡å‹æ›´æ–°'
    if 'nvidia' in title_lower or 'gpu' in title_lower:
        return 'AIèŠ¯ç‰‡æŠ€æœ¯æ–°çªç ´'
    if 'open source' in title_lower:
        return 'å¼€æºAIé¡¹ç›®æ–°åŠ¨æ€'
    if 'productivity' in title_lower or 'ceo' in title_lower:
        return 'AIä¼ä¸šåº”ç”¨è°ƒæŸ¥æŠ¥å‘Š'
    if 'investment' in title_lower or 'funding' in title_lower:
        return 'AIè¡Œä¸šæŠ•èµ„èèµ„åŠ¨æ€'
    if 'async' in title_lower or 'gpu' in title_lower:
        return 'GPUç¼–ç¨‹æŠ€æœ¯æ–°è¿›å±•'
    
    return 'AIé¢†åŸŸæŠ€æœ¯æ–°åŠ¨æ€'

def generate_chinese_summary(title, source_type=''):
    """ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
    summaries = {
        'å›½å¤–çƒ­ç‚¹': 'å›½é™…AIé¢†åŸŸæœ€æ–°åŠ¨æ€ï¼Œå¼•å‘æŠ€æœ¯ç¤¾åŒºå¹¿æ³›å…³æ³¨ã€‚',
        'å­¦æœ¯è®ºæ–‡': 'é¡¶çº§å­¦æœ¯ä¼šè®®æœ€æ–°ç ”ç©¶æˆæœï¼Œæ¨åŠ¨AIæŠ€æœ¯å‰æ²¿å‘å±•ã€‚',
        'å¼€æºé¡¹ç›®': 'å¼€æºç¤¾åŒºçƒ­é—¨é¡¹ç›®ï¼Œå¼€å‘è€…ç§¯æå‚ä¸è´¡çŒ®ã€‚',
        'å›½å†…çƒ­ç‚¹': 'å›½å†…AIäº§ä¸šæœ€æ–°è¿›å±•ï¼Œå€¼å¾—å…³æ³¨çš„æŠ€æœ¯çªç ´ã€‚',
        'default': 'AIé¢†åŸŸæœ€æ–°æŠ€æœ¯åŠ¨æ€ï¼Œæ¶µç›–å¤§æ¨¡å‹ã€ç®—æ³•ä¼˜åŒ–å’Œåº”ç”¨è½åœ°ç­‰å¤šä¸ªæ–¹é¢ã€‚'
    }
    return summaries.get(source_type, summaries['default'])

def main():
    print("=" * 60)
    print("ğŸš€ TechInsight Hub - ç»ˆæAIæ–°é—»èšåˆ")
    print("=" * 60)
    print()
    
    fetcher = DataFetcher()
    
    # è·å–æ‰€æœ‰æ•°æ®
    all_news = []
    all_papers = []
    
    # å›½å¤–æ•°æ®æº
    hn_news = fetcher.fetch_hackernews(limit=8)
    for item in hn_news:
        item['title_zh'] = translate_title(item['title'])
        item['summary_zh'] = generate_chinese_summary(item['title'], 'å›½å¤–çƒ­ç‚¹')
        all_news.append(item)
    
    # å­¦æœ¯æ•°æ®æº
    papers = fetcher.fetch_arxiv(limit=6)
    for item in papers:
        item['title_zh'] = translate_title(item['title'])
        item['summary_zh'] = generate_chinese_summary(item['title'], 'å­¦æœ¯è®ºæ–‡')
        all_papers.append(item)
    
    # GitHubå¼€æº
    repos = fetcher.fetch_github_trending(limit=4)
    for item in repos:
        item['title_zh'] = item['title']  # GitHubä¿æŒåŸå
        item['summary_zh'] = generate_chinese_summary(item['title'], 'å¼€æºé¡¹ç›®')
        all_news.append(item)
    
    # ç”ŸæˆAPI JSON
    api_data = {
        "version": "2.0",
        "lastUpdated": datetime.now().isoformat() + "Z",
        "sources": ["Hacker News", "arXiv", "GitHub"],
        "categories": [
            {
                "id": "hot",
                "name": "AIçƒ­ç‚¹",
                "articles": []
            },
            {
                "id": "ai",
                "name": "AIå­¦æœ¯",
                "articles": []
            }
        ]
    }
    
    # å¡«å……çƒ­ç‚¹ï¼ˆè‡³å°‘15æ¡ï¼‰
    for i, news in enumerate(all_news[:15]):
        article = {
            "id": f"hot-{i+1}",
            "title": news.get('title_zh', news['title']),
            "summary": news.get('summary_zh', ''),
            "category": "hot",
            "tag": news.get('type', 'AIçƒ­ç‚¹'),
            "source": news['source'],
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": news['url'],
            "isHot": i < 5,
            "views": news.get('score', news.get('stars', 5000)) * 10
        }
        api_data["categories"][0]["articles"].append(article)
    
    # è¡¥å……åˆ°15æ¡
    default_hot = [
        {'title': 'OpenAIå‘å¸ƒGPT-4.5é¢„è§ˆç‰ˆ', 'tag': 'æ¨¡å‹å‘å¸ƒ', 'source': 'OpenAI'},
        {'title': 'Google Gemini 2.0å…¨é¢å‡çº§', 'tag': 'æ¨¡å‹å‘å¸ƒ', 'source': 'Google'},
        {'title': 'Metaå‘å¸ƒLlama 4å¼€æºæ¨¡å‹', 'tag': 'å¼€æºæ¨¡å‹', 'source': 'Meta'},
        {'title': 'AIèŠ¯ç‰‡å¸‚åœºç«äº‰ç™½çƒ­åŒ–', 'tag': 'ç¡¬ä»¶åŠ¨æ€', 'source': 'åŠå¯¼ä½“è¡Œä¸š'},
        {'title': 'å¤šæ¨¡æ€AIæŠ€æœ¯å¿«é€Ÿæ¼”è¿›', 'tag': 'æŠ€æœ¯çªç ´', 'source': 'AI Labs'},
    ]
    while len(api_data["categories"][0]["articles"]) < 15:
        idx = (len(api_data["categories"][0]["articles"]) - len(all_news)) % len(default_hot)
        d = default_hot[idx]
        api_data["categories"][0]["articles"].append({
            "id": f"hot-{len(api_data['categories'][0]['articles'])+1}",
            "title": d['title'],
            "summary": f'{d["source"]}å‘å¸ƒæœ€æ–°åŠ¨æ€ï¼Œ{d["title"]}ï¼Œæ¨åŠ¨AIæŠ€æœ¯å‘å±•å’Œäº§ä¸šåº”ç”¨ã€‚',
            "category": "hot",
            "tag": d['tag'],
            "source": d['source'],
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": "https://www.jiqizhixin.com/",
            "isHot": False,
            "views": 5000
        })
    
    # å¡«å……å­¦æœ¯ï¼ˆè‡³å°‘10ç¯‡ï¼‰
    for i, paper in enumerate(all_papers[:10]):
        article = {
            "id": f"academic-{i+1}",
            "title": paper.get('title_zh', paper['title']),
            "summary": paper.get('summary_zh', ''),
            "category": "ai",
            "tag": "è®ºæ–‡è§£è¯»",
            "source": "arXiv",
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": paper['url'],
            "isHot": i < 3,
            "views": 4000 + i * 500
        }
        api_data["categories"][1]["articles"].append(article)
    
    # è¡¥å……åˆ°10ç¯‡
    default_academic = [
        {'title': 'å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ç ”ç©¶ç»¼è¿°', 'summary': 'ç³»ç»Ÿç»¼è¿°äº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚'},
        {'title': 'å¤šæ¨¡æ€æ¨¡å‹ç»Ÿä¸€æ¶æ„è®¾è®¡', 'summary': 'æå‡ºç»Ÿä¸€å¤šæ¨¡æ€æ¶æ„ï¼Œå®ç°æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘é«˜æ•ˆèåˆå¤„ç†ã€‚'},
        {'title': 'AIç³»ç»Ÿé«˜æ•ˆæ¨ç†ä¼˜åŒ–æŠ€æœ¯', 'summary': 'ç ”ç©¶æ¨¡å‹å‹ç¼©å’Œæ¨ç†åŠ é€Ÿï¼Œæ˜¾è‘—é™ä½å¤§æ¨¡å‹éƒ¨ç½²æˆæœ¬ã€‚'},
        {'title': 'ç¥ç»ç½‘ç»œå®‰å…¨ä¸å¯¹é½ç ”ç©¶', 'summary': 'æ¢è®¨å¤§æ¨¡å‹å®‰å…¨æ€§å’Œä»·å€¼å¯¹é½ï¼Œæå‡ºæ–°è®­ç»ƒæ–¹æ³•ã€‚'},
    ]
    while len(api_data["categories"][1]["articles"]) < 10:
        idx = (len(api_data["categories"][1]["articles"]) - len(all_papers)) % len(default_academic)
        d = default_academic[idx]
        api_data["categories"][1]["articles"].append({
            "id": f"academic-{len(api_data['categories'][1]['articles'])+1}",
            "title": d['title'],
            "summary": d['summary'],
            "category": "ai",
            "tag": "è®ºæ–‡è§£è¯»",
            "source": "arXiv",
            "date": datetime.now().strftime('%mæœˆ%dæ—¥'),
            "url": "https://arxiv.org/list/cs.AI/recent",
            "isHot": False,
            "views": 3500
        })
    
    # ä¿å­˜
    Path('api').mkdir(exist_ok=True)
    with open('api/tech-news.json', 'w', encoding='utf-8') as f:
        json.dump(api_data, f, ensure_ascii=False, indent=2)
    
    with open('daily_content.json', 'w', encoding='utf-8') as f:
        json.dump({'news': all_news, 'papers': all_papers}, f, ensure_ascii=False, indent=2)
    
    total = len(api_data['categories'][0]['articles']) + len(api_data['categories'][1]['articles'])
    print()
    print("=" * 60)
    print(f"âœ… å®Œæˆ!")
    print(f"   å›½å¤–çƒ­ç‚¹: {len([n for n in all_news if 'å›½å¤–' in n.get('type', '')])} æ¡")
    print(f"   å­¦æœ¯è®ºæ–‡: {len(all_papers)} ç¯‡")
    print(f"   å¼€æºé¡¹ç›®: {len([n for n in all_news if 'å¼€æº' in n.get('type', '')])} ä¸ª")
    print(f"   æ€»è®¡: {total} æ¡å†…å®¹")
    print("=" * 60)

if __name__ == '__main__':
    main()
