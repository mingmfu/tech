#!/usr/bin/env python3
"""
ç”Ÿæˆ tech-news.json API æ•°æ®æ–‡ä»¶
ä¾›æ‰‹æœºç«¯åº”ç”¨è®¿é—®
"""

import json
import uuid
from datetime import datetime
from pathlib import Path

def generate_id(title):
    """ä»æ ‡é¢˜ç”ŸæˆID"""
    return title.lower().replace(' ', '-').replace(':', '').replace('(', '').replace(')', '')[:50]

def generate_tech_news():
    """ç”Ÿæˆ tech-news.json"""
    
    # è¯»å–æŠ“å–çš„æ•°æ®
    with open('daily_content.json', 'r', encoding='utf-8') as f:
        daily_data = json.load(f)
    
    # æ„å»ºAPIæ•°æ®ç»“æ„
    tech_news = {
        "version": "1.0",
        "lastUpdated": datetime.now().isoformat() + "Z",
        "categories": [
            {
                "id": "hot",
                "name": "AIçƒ­ç‚¹",
                "articles": []
            },
            {
                "id": "ai",
                "name": "AIå­¦æœ¯", 
                "articles": []
            }
        ]
    }
    
    # å¤„ç†çƒ­ç‚¹æ–°é—»
    for i, news in enumerate(daily_data.get('news', [])[:6]):
        article = {
            "id": generate_id(news['title']),
            "title": news['title'],
            "summary": news.get('summary', news['title']),
            "category": "hot",
            "tag": news.get('tag', 'çƒ­ç‚¹'),
            "source": news.get('source', news.get('meta', ['Unknown'])[0].replace('ğŸ”¥ ', '').replace('â­ ', '')),
            "date": datetime.now().strftime('%Y-%m-%d'),
            "url": news['url'],
            "isHot": i < 3,  # å‰3æ¡æ ‡è®°ä¸ºçƒ­é—¨
            "views": news.get('score', 0) * 10 if 'score' in news else 5000
        }
        tech_news["categories"][0]["articles"].append(article)
    
    # å¤„ç†å­¦æœ¯å†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰è®ºæ–‡ï¼Œä½¿ç”¨é»˜è®¤å†…å®¹ï¼‰
    default_academic = [
        {
            "title": "DeepSeek-R1: æ¨ç†æ¨¡å‹çš„å¼€æºçªç ´",
            "summary": "DeepSeek å‘å¸ƒçš„ R1 æ¨¡å‹åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šåª²ç¾ OpenAI o1ï¼Œä»¥æä½çš„è®­ç»ƒæˆæœ¬å®ç°äº†æƒŠäººçš„æ€§èƒ½ã€‚",
            "tag": "LLM",
            "source": "arXiv",
            "url": "https://arxiv.org/abs/2501.12948",
            "isHot": True,
            "views": 25678
        },
        {
            "title": "Mixture of Experts (MoE) æ¶æ„æ–°è¿›å±•",
            "summary": "æœ€æ–°çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±å’Œä¸“å®¶é€‰æ‹©ç­–ç•¥çš„ä¼˜åŒ–ï¼ŒMoE æ¨¡å‹å¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å°†æ¨ç†æˆæœ¬é™ä½ 40%ã€‚",
            "tag": "LLM Â· Efficiency",
            "source": "arXiv",
            "url": "https://arxiv.org/abs/2502.08934",
            "isHot": False,
            "views": 5432
        },
        {
            "title": "Sora ä¹‹åï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æŠ€æœ¯æ¼”è¿›",
            "summary": "OpenAI Sora å±•ç¤ºäº† Transformer åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚æœ€æ–°ç ”ç©¶èšç„¦äºæ—¶ç©ºä¸€è‡´æ€§ã€é•¿è§†é¢‘ç”Ÿæˆå’Œå¯æ§æ€§ã€‚",
            "tag": "Vision Â· Multimodal",
            "source": "arXiv",
            "url": "https://arxiv.org/abs/2502.07652",
            "isHot": False,
            "views": 7654
        },
        {
            "title": "Agent ç³»ç»Ÿçš„è®°å¿†æœºåˆ¶è®¾è®¡",
            "summary": "å¦‚ä½•è®© AI Agent æ‹¥æœ‰é•¿æœŸè®°å¿†ï¼Ÿæœ€æ–°çš„è®°å¿†æ¶æ„ç»“åˆäº†å‘é‡æ£€ç´¢ã€çŸ¥è¯†å›¾è°±å’Œå‚æ•°è®°å¿†ã€‚",
            "tag": "Agents Â· RAG",
            "source": "arXiv",
            "url": "https://arxiv.org/abs/2502.06731",
            "isHot": False,
            "views": 4321
        },
        {
            "title": "AI èŠ¯ç‰‡çš„å­˜ç®—ä¸€ä½“æ–°æ¶æ„",
            "summary": "å­˜å†…è®¡ç®— (Compute-in-Memory) æŠ€æœ¯æ­£åœ¨æˆç†Ÿï¼Œå¯å°† Transformer æ¨ç†èƒ½è€—é™ä½ 10 å€ã€‚",
            "tag": "MLOps Â· Hardware",
            "source": "ISSCC",
            "url": "https://arxiv.org/abs/2501.18485",
            "isHot": False,
            "views": 3456
        }
    ]
    
    papers = daily_data.get('papers', [])
    if papers:
        # ä½¿ç”¨å®é™…è·å–çš„è®ºæ–‡
        for i, paper in enumerate(papers[:5]):
            article = {
                "id": generate_id(paper['title']),
                "title": paper['title'],
                "summary": f"{paper['category']} æœ€æ–°è®ºæ–‡ç ”ç©¶",
                "category": "ai",
                "tag": paper.get('category', 'AI').replace('cs.', ''),
                "source": "arXiv",
                "date": paper.get('date', datetime.now().strftime('%Y-%m-%d')),
                "url": paper.get('url', f"https://arxiv.org/abs/{paper.get('arxiv_id', '')}"),
                "isHot": i < 1,
                "views": 5000 + i * 1000
            }
            tech_news["categories"][1]["articles"].append(article)
    else:
        # ä½¿ç”¨é»˜è®¤å­¦æœ¯å†…å®¹
        for i, article_data in enumerate(default_academic):
            article = {
                "id": generate_id(article_data['title']),
                "title": article_data['title'],
                "summary": article_data['summary'],
                "category": "ai",
                "tag": article_data['tag'],
                "source": article_data['source'],
                "date": datetime.now().strftime('%Y-%m-%d'),
                "url": article_data['url'],
                "isHot": article_data['isHot'],
                "views": article_data['views']
            }
            tech_news["categories"][1]["articles"].append(article)
    
    # ä¿å­˜åˆ°apiæ–‡ä»¶å¤¹
    api_dir = Path('api')
    api_dir.mkdir(exist_ok=True)
    
    output_file = api_dir / 'tech-news.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(tech_news, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ç”Ÿæˆ {output_file}")
    print(f"   - AIçƒ­ç‚¹: {len(tech_news['categories'][0]['articles'])} æ¡")
    print(f"   - AIå­¦æœ¯: {len(tech_news['categories'][1]['articles'])} æ¡")
    
    return tech_news

if __name__ == '__main__':
    generate_tech_news()
