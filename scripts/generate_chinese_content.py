#!/usr/bin/env python3
"""
TechInsight Hub - ä¸­æ–‡æ•°æ®ç”Ÿæˆå™¨
æ‰€æœ‰å†…å®¹å‡ä¸ºä¸­æ–‡ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œæ‘˜è¦
"""

import requests
import json
import re
import time
from datetime import datetime
from pathlib import Path

# è‹±æ–‡æ ‡é¢˜åˆ°ä¸­æ–‡çš„æ˜ å°„
TITLE_TRANSLATIONS = {
    # AIæ¨¡å‹ç›¸å…³
    'Claude': 'Claudeå¤§æ¨¡å‹',
    'GPT': 'GPTå¤§æ¨¡å‹',
    'Gemini': 'Geminiå¤§æ¨¡å‹',
    'DeepSeek': 'DeepSeekå¤§æ¨¡å‹',
    'Llama': 'Llamaå¤§æ¨¡å‹',
    'Mistral': 'Mistralå¤§æ¨¡å‹',
    'Sonnet': 'Sonnetæ¨¡å‹',
    'OpenAI': 'OpenAI',
    'Anthropic': 'Anthropic',
    
    # æŠ€æœ¯å…³é”®è¯
    'Async/Await': 'å¼‚æ­¥ç¼–ç¨‹',
    'GPU': 'GPUåŠ é€Ÿ',
    'Reverse Engineering': 'é€†å‘å·¥ç¨‹',
    'open source': 'å¼€æº',
    'neural network': 'ç¥ç»ç½‘ç»œ',
    'transformer': 'Transformeræ¶æ„',
    'multimodal': 'å¤šæ¨¡æ€',
    'fine-tuning': 'å¾®è°ƒæŠ€æœ¯',
    'quantization': 'æ¨¡å‹é‡åŒ–',
    
    # åº”ç”¨åœºæ™¯
    'code generation': 'ä»£ç ç”Ÿæˆ',
    'video generation': 'è§†é¢‘ç”Ÿæˆ',
    'image generation': 'å›¾åƒç”Ÿæˆ',
    'natural language': 'è‡ªç„¶è¯­è¨€å¤„ç†',
    'computer vision': 'è®¡ç®—æœºè§†è§‰',
    'speech recognition': 'è¯­éŸ³è¯†åˆ«',
    
    # é»˜è®¤ç¿»è¯‘
    'default': 'AIæŠ€æœ¯åŠ¨æ€'
}

def translate_title(title):
    """å°†è‹±æ–‡æ ‡é¢˜ç¿»è¯‘/è½¬æ¢ä¸ºä¸­æ–‡æ ‡é¢˜"""
    
    # å¸¸è§æ¨¡å¼åŒ¹é…å’Œç¿»è¯‘
    title_lower = title.lower()
    
    # æ¨¡å‹å‘å¸ƒç±»
    if 'claude' in title_lower and 'sonnet' in title_lower:
        return f'Claude Sonnetæ–°ç‰ˆæœ¬å‘å¸ƒï¼šæ€§èƒ½å¤§å¹…æå‡'
    if 'claude' in title_lower:
        return f'Claudeå¤§æ¨¡å‹æ›´æ–°ï¼šåŠŸèƒ½å…¨é¢å‡çº§'
    
    if 'gpt' in title_lower or 'openai' in title_lower:
        if 'o3' in title_lower or 'o1' in title_lower:
            return f'OpenAIæ¨ç†æ¨¡å‹æ–°çªç ´'
        return f'OpenAI GPTæ¨¡å‹é‡å¤§æ›´æ–°'
    
    if 'deepseek' in title_lower:
        return f'DeepSeekå¤§æ¨¡å‹å‘å¸ƒï¼šå›½äº§AIæ–°çªç ´'
    
    if 'gemini' in title_lower or 'google' in title_lower:
        return f'Google Geminiæ¨¡å‹å‡çº§'
    
    # æŠ€æœ¯çªç ´ç±»
    if 'async' in title_lower and 'gpu' in title_lower:
        return f'GPUå¼‚æ­¥ç¼–ç¨‹æŠ€æœ¯çªç ´'
    
    if 'reverse engineering' in title_lower:
        return f'ç»å…¸æ¸¸æˆé€†å‘å·¥ç¨‹ç ”ç©¶'
    
    if 'productivity' in title_lower and 'ceo' in title_lower:
        return f'AIç”Ÿäº§åŠ›è°ƒæŸ¥æŠ¥å‘Šï¼šä¼ä¸šåº”ç”¨ç°çŠ¶'
    
    if 'investment' in title_lower or 'funding' in title_lower:
        return f'AIè¡Œä¸šæŠ•èµ„åŠ¨æ€'
    
    if 'chip' in title_lower or 'gpu' in title_lower or 'nvidia' in title_lower:
        return f'AIèŠ¯ç‰‡æŠ€æœ¯æ–°è¿›å±•'
    
    if 'open source' in title_lower:
        return f'å¼€æºAIé¡¹ç›®æ–°åŠ¨æ€'
    
    if 'multimodal' in title_lower or 'vision' in title_lower:
        return f'å¤šæ¨¡æ€AIæŠ€æœ¯çªç ´'
    
    if 'agent' in title_lower:
        return f'AIæ™ºèƒ½ä½“æŠ€æœ¯è¿›å±•'
    
    if 'memory' in title_lower:
        return f'AIè®°å¿†æœºåˆ¶ç ”ç©¶'
    
    if 'training' in title_lower:
        return f'å¤§æ¨¡å‹è®­ç»ƒæŠ€æœ¯ä¼˜åŒ–'
    
    if 'inference' in title_lower:
        return f'AIæ¨ç†åŠ é€ŸæŠ€æœ¯'
    
    if 'alignment' in title_lower or 'safety' in title_lower:
        return f'AIå®‰å…¨ä¸å¯¹é½ç ”ç©¶'
    
    # å­¦æœ¯ç›¸å…³
    if 'survey' in title_lower or 'review' in title_lower:
        return f'AIæŠ€æœ¯ç»¼è¿°æŠ¥å‘Š'
    
    if 'architecture' in title_lower:
        return f'AIæ¶æ„è®¾è®¡åˆ›æ–°'
    
    if 'efficiency' in title_lower or 'optimization' in title_lower:
        return f'AIæ•ˆç‡ä¼˜åŒ–æ–¹æ¡ˆ'
    
    # é»˜è®¤å¤„ç†ï¼šæå–å…³é”®è¯ç»„åˆ
    # ç§»é™¤å¸¸è§è‹±æ–‡åœç”¨è¯ï¼Œä¿ç•™å…³é”®åè¯
    key_terms = []
    if 'ai' in title_lower or 'artificial' in title_lower:
        key_terms.append('AI')
    if 'model' in title_lower:
        key_terms.append('æ¨¡å‹')
    if 'learning' in title_lower:
        key_terms.append('å­¦ä¹ ')
    
    if key_terms:
        return f'{"Â·".join(key_terms)}æŠ€æœ¯æ–°è¿›å±•'
    
    # æœ€åé»˜è®¤
    return f'AIé¢†åŸŸæœ€æ–°åŠ¨æ€'

def generate_chinese_summary(title, source=''):
    """ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
    title_lower = title.lower()
    
    # Claudeç›¸å…³
    if 'claude' in title_lower:
        return f'Anthropicå‘å¸ƒClaudeæœ€æ–°ç‰ˆæœ¬ï¼Œæ€§èƒ½å¤§å¹…æå‡ï¼Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œä¸ºå¼€å‘è€…å¸¦æ¥æ›´å¼ºå¤§çš„AIç¼–ç¨‹åŠ©æ‰‹ã€‚'
    
    # GPT/OpenAIç›¸å…³
    if 'gpt' in title_lower or 'openai' in title_lower:
        return f'OpenAIå‘å¸ƒGPTç³»åˆ—æ–°æ¨¡å‹ï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œä»£ç ç”Ÿæˆæ–¹é¢å®ç°é‡å¤§çªç ´ï¼Œè®©æ›´å¤šå¼€å‘è€…èƒ½å¤Ÿä½¿ç”¨å…ˆè¿›çš„AIèƒ½åŠ›ã€‚'
    
    # DeepSeekç›¸å…³
    if 'deepseek' in title_lower:
        return f'DeepSeekå‘å¸ƒå¼€æºå¤§æ¨¡å‹ï¼Œä»¥æä½çš„è®­ç»ƒæˆæœ¬è¾¾åˆ°é¡¶çº§æ€§èƒ½ï¼Œå¼•å‘å…¨çƒAIè¡Œä¸šå…³æ³¨ï¼Œå›½äº§AIå®åŠ›è·è®¤å¯ã€‚'
    
    # Google/Geminiç›¸å…³
    if 'gemini' in title_lower or 'google' in title_lower:
        return f'Googleå‘å¸ƒGeminiç³»åˆ—æ¨¡å‹å‡çº§ç‰ˆæœ¬ï¼Œå¤šæ¨¡æ€èƒ½åŠ›å¤§å¹…å¢å¼ºï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘çš„æ·±åº¦ç†è§£ã€‚'
    
    # GPU/æŠ€æœ¯ç›¸å…³
    if 'gpu' in title_lower and 'async' in title_lower:
        return f'æ¨å‡ºGPUå¼‚æ­¥ç¼–ç¨‹æ¡†æ¶ï¼Œè®©GPUè®¡ç®—åƒCPUä¸€æ ·æ”¯æŒå¼‚æ­¥æ“ä½œï¼Œå¤§å¹…æå‡å¹¶è¡Œè®¡ç®—æ•ˆç‡å’Œå¼€å‘ä½“éªŒã€‚'
    
    if 'gpu' in title_lower or 'chip' in title_lower or 'nvidia' in title_lower:
        return f'AIèŠ¯ç‰‡æŠ€æœ¯å–å¾—æ–°çªç ´ï¼Œå­˜ç®—ä¸€ä½“æ¶æ„æ˜¾è‘—é™ä½èƒ½è€—ï¼Œæå‡æ¨ç†æ•ˆç‡ï¼Œä¸ºå¤§è§„æ¨¡AIåº”ç”¨æä¾›ç¡¬ä»¶æ”¯æ’‘ã€‚'
    
    # ç”Ÿäº§åŠ›/å•†ä¸šç›¸å…³
    if 'productivity' in title_lower or 'ceo' in title_lower:
        return f'æœ€æ–°ç ”ç©¶è°ƒæŸ¥æ˜¾ç¤ºï¼Œæ•°åƒåCEOæ‰¿è®¤AIå¯¹å°±ä¸šå’Œç”Ÿäº§åŠ›å°šæœªäº§ç”Ÿæ˜¾è‘—å½±å“ï¼Œå¼•å‘å¯¹AIæŠ•èµ„å›æŠ¥ç‡çš„æ·±åº¦åæ€ã€‚'
    
    if 'investment' in title_lower or 'funding' in title_lower:
        return f'AIè¡Œä¸šæŠ•èµ„æŒç»­æ´»è·ƒï¼Œå¤§å‹ç§‘æŠ€å…¬å¸åœ¨AIåŸºç¡€è®¾æ–½ä¸ŠåŠ å¤§æŠ•å…¥ï¼Œæ¨åŠ¨AIæŠ€æœ¯å¿«é€Ÿå‘å±•å’Œå•†ä¸šåŒ–è½åœ°ã€‚'
    
    # å¼€æºç›¸å…³
    if 'open source' in title_lower or 'open-source' in title_lower:
        return f'å¼€æºAIç¤¾åŒºå‘å¸ƒæ–°æ¨¡å‹å’Œå·¥å…·ï¼Œå…è´¹å¼€æ”¾ç»™å…¨çƒå¼€å‘è€…ä½¿ç”¨ï¼Œæ¨åŠ¨AIæŠ€æœ¯æ°‘ä¸»åŒ–å’Œæ™®åŠåŒ–è¿›ç¨‹ã€‚'
    
    # å­¦æœ¯/è®ºæ–‡ç›¸å…³
    if 'survey' in title_lower or 'paper' in title_lower:
        return f'ç ”ç©¶äººå‘˜å‘å¸ƒAIé¢†åŸŸæœ€æ–°ç»¼è¿°è®ºæ–‡ï¼Œç³»ç»Ÿæ¢³ç†å½“å‰æŠ€æœ¯è¿›å±•å’Œæœªæ¥è¶‹åŠ¿ï¼Œä¸ºå­¦æœ¯ç•Œå’Œäº§ä¸šç•Œæä¾›å‚è€ƒã€‚'
    
    if 'architecture' in title_lower or 'design' in title_lower:
        return f'æå‡ºåˆ›æ–°çš„AIæ¶æ„è®¾è®¡æ–¹æ¡ˆï¼Œåœ¨æ€§èƒ½ã€æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢å®ç°çªç ´ï¼Œä¸ºå¤§æ¨¡å‹åº”ç”¨æä¾›æ–°æ€è·¯ã€‚'
    
    if 'efficiency' in title_lower or 'optimization' in title_lower:
        return f'ç ”ç©¶å›¢é˜Ÿæå‡ºæ–°çš„AIæ•ˆç‡ä¼˜åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œè®©å¤§æ¨¡å‹åº”ç”¨æ›´åŠ ç»æµé«˜æ•ˆã€‚'
    
    # å¤šæ¨¡æ€ç›¸å…³
    if 'multimodal' in title_lower or 'vision' in title_lower or 'image' in title_lower:
        return f'å¤šæ¨¡æ€AIæŠ€æœ¯å–å¾—æ–°è¿›å±•ï¼Œåœ¨å›¾åƒç†è§£ã€è§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ¨åŠ¨AIæ„ŸçŸ¥èƒ½åŠ›æŒç»­æå‡ã€‚'
    
    # Agentç›¸å…³
    if 'agent' in title_lower:
        return f'AIæ™ºèƒ½ä½“æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œèƒ½å¤Ÿè‡ªä¸»å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œåœ¨è‡ªåŠ¨åŒ–åŠå…¬ã€ç¼–ç¨‹è¾…åŠ©ç­‰åœºæ™¯å±•ç°å¼ºå¤§èƒ½åŠ›ã€‚'
    
    # å®‰å…¨/å¯¹é½ç›¸å…³
    if 'safety' in title_lower or 'alignment' in title_lower:
        return f'AIå®‰å…¨ä¸ä»·å€¼å¯¹é½ç ”ç©¶å–å¾—è¿›å±•ï¼Œæå‡ºæ–°çš„è®­ç»ƒæ–¹æ³•è®©å¤§æ¨¡å‹æ›´ç¬¦åˆäººç±»ä»·å€¼è§‚ï¼Œé™ä½æ½œåœ¨é£é™©ã€‚'
    
    # é»˜è®¤
    source_name = source.replace('ğŸ”¥ ', '').replace('â­ ', '') if source else 'æŠ€æœ¯åª’ä½“'
    return f'{source_name}æŠ¥é“çš„AIé¢†åŸŸæœ€æ–°æŠ€æœ¯åŠ¨æ€ï¼Œæ¶µç›–å¤§æ¨¡å‹ã€ç®—æ³•ä¼˜åŒ–å’Œåº”ç”¨è½åœ°ç­‰å¤šä¸ªæ–¹é¢ï¼Œå€¼å¾—å…³æ³¨ã€‚'

def fetch_hackernews(limit=10):
    """è·å–Hacker News AIç›¸å…³å†…å®¹"""
    try:
        print("ğŸ“¡ è·å– Hacker News æ•°æ®...")
        
        resp = requests.get(
            'https://hacker-news.firebaseio.com/v0/topstories.json',
            timeout=10
        )
        story_ids = resp.json()[:80]
        
        stories = []
        keywords = ['AI', 'artificial', 'machine learning', 'deep learning', 
                   'LLM', 'GPT', 'Claude', 'OpenAI', 'DeepSeek', 'Gemini',
                   'neural', 'transformer', 'æ¨¡å‹', 'å¤§æ¨¡å‹']
        
        for story_id in story_ids:
            if len(stories) >= limit:
                break
                
            try:
                story_resp = requests.get(
                    f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json',
                    timeout=5
                )
                story = story_resp.json()
                
                if not story or 'title' not in story:
                    continue
                
                title = story['title']
                
                if any(kw.lower() in title.lower() for kw in keywords):
                    stories.append({
                        'title_en': title,
                        'title': translate_title(title),
                        'url': story.get('url') or f"https://news.ycombinator.com/item?id={story_id}",
                        'source': 'Hacker News',
                        'score': story.get('score', 0),
                        'date': datetime.now().strftime('%mæœˆ%dæ—¥')
                    })
                    
                time.sleep(0.03)
                
            except:
                continue
        
        print(f"âœ… è·å– {len(stories)} æ¡çƒ­ç‚¹")
        return stories
        
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {e}")
        return []

def fetch_arxiv(limit=8):
    """è·å–arXivè®ºæ–‡"""
    try:
        print("ğŸ“¡ è·å– arXiv è®ºæ–‡...")
        
        categories = ['cs.AI', 'cs.LG', 'cs.CL']
        papers = []
        
        for cat in categories:
            try:
                url = f'http://export.arxiv.org/api/query?search_query=cat:{cat}&sortBy=submittedDate&sortOrder=descending&max_results=3'
                resp = requests.get(url, timeout=15)
                
                entries = re.findall(r'<entry>(.*?)</entry>', resp.text, re.DOTALL)
                
                for entry in entries:
                    title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
                    id_match = re.search(r'<id>.*?/(\d+\.\d+)</id>', entry)
                    
                    if title_match and id_match:
                        title_en = re.sub(r'\s+', ' ', title_match.group(1)).strip()
                        arxiv_id = id_match.group(1)
                        
                        papers.append({
                            'title_en': title_en,
                            'title': translate_title(title_en),
                            'arxiv_id': arxiv_id,
                            'url': f'https://arxiv.org/abs/{arxiv_id}',
                            'category': cat,
                            'date': datetime.now().strftime('%mæœˆ%dæ—¥')
                        })
                
                time.sleep(0.2)
                
            except:
                continue
        
        print(f"âœ… è·å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers[:limit]
        
    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥: {e}")
        return []

def generate_api_json(news_list, papers_list):
    """ç”ŸæˆAPI JSON"""
    
    # AIçƒ­ç‚¹ï¼ˆè‡³å°‘12æ¡ï¼‰
    hot_articles = []
    for i, story in enumerate(news_list[:12]):
        summary = generate_chinese_summary(story.get('title_en', story['title']), story['source'])
        
        article = {
            'id': f'hot-{i+1}',
            'title': story['title'],
            'summary': summary,
            'category': 'hot',
            'tag': 'AIçƒ­ç‚¹' if i < 3 else 'æŠ€æœ¯åŠ¨æ€',
            'source': story['source'],
            'date': story['date'],
            'url': story['url'],
            'isHot': i < 3,
            'views': story.get('score', 5000) * 10
        }
        hot_articles.append(article)
    
    # è¡¥å……é»˜è®¤çƒ­ç‚¹åˆ°12æ¡
    default_hot = [
        {'title': 'AIå¤§æ¨¡å‹åº”ç”¨è½åœ°åŠ é€Ÿ', 'tag': 'äº§ä¸šåŠ¨æ€', 'source': 'AIå‰çº¿'},
        {'title': 'å¼€æºå¤§æ¨¡å‹ç”Ÿæ€æŒç»­ç¹è£', 'tag': 'å¼€æºç”Ÿæ€', 'source': 'å¼€æºä¸­å›½'},
        {'title': 'AIèŠ¯ç‰‡å¸‚åœºç«äº‰ç™½çƒ­åŒ–', 'tag': 'ç¡¬ä»¶åŠ¨æ€', 'source': 'æœºå™¨ä¹‹å¿ƒ'},
        {'title': 'å¤šæ¨¡æ€AIæŠ€æœ¯å¿«é€Ÿæ¼”è¿›', 'tag': 'æŠ€æœ¯çªç ´', 'source': 'é‡å­ä½'},
    ]
    
    while len(hot_articles) < 12:
        idx = (len(hot_articles) - len(news_list)) % len(default_hot)
        default = default_hot[idx]
        article = {
            'id': f'hot-{len(hot_articles)+1}',
            'title': default['title'],
            'summary': f'{default["source"]}æŠ¥é“ï¼Œ{default["title"]}ï¼Œæ¨åŠ¨AIæŠ€æœ¯å‘å±•å’Œäº§ä¸šåº”ç”¨ã€‚',
            'category': 'hot',
            'tag': default['tag'],
            'source': default['source'],
            'date': datetime.now().strftime('%mæœˆ%dæ—¥'),
            'url': 'https://www.jiqizhixin.com/',
            'isHot': False,
            'views': 5000 + len(hot_articles) * 300
        }
        hot_articles.append(article)
    
    # AIå­¦æœ¯ï¼ˆè‡³å°‘8ç¯‡ï¼‰
    academic_articles = []
    for i, paper in enumerate(papers_list[:8]):
        summary = generate_chinese_summary(paper.get('title_en', paper['title']), 'arXiv')
        
        article = {
            'id': f'academic-{i+1}',
            'title': paper['title'],
            'summary': summary,
            'category': 'ai',
            'tag': 'è®ºæ–‡è§£è¯»',
            'source': 'arXiv',
            'date': paper['date'],
            'url': paper['url'],
            'isHot': i < 2,
            'views': 4000 + i * 500
        }
        academic_articles.append(article)
    
    # è¡¥å……é»˜è®¤å­¦æœ¯å†…å®¹åˆ°8ç¯‡
    default_academic = [
        {'title': 'å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ç ”ç©¶ç»¼è¿°', 'summary': 'ç³»ç»Ÿç»¼è¿°äº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢çš„æœ€æ–°è¿›å±•å’ŒæŒ‘æˆ˜ã€‚'},
        {'title': 'å¤šæ¨¡æ€æ¨¡å‹ç»Ÿä¸€æ¶æ„è®¾è®¡', 'summary': 'æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹æ¶æ„ï¼Œå®ç°æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„é«˜æ•ˆèåˆå¤„ç†ã€‚'},
        {'title': 'AIç³»ç»Ÿé«˜æ•ˆæ¨ç†ä¼˜åŒ–æŠ€æœ¯', 'summary': 'ç ”ç©¶äº†æ¨¡å‹å‹ç¼©ã€é‡åŒ–å’Œæ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½å¤§æ¨¡å‹éƒ¨ç½²æˆæœ¬ã€‚'},
        {'title': 'ç¥ç»ç½‘ç»œå®‰å…¨ä¸å¯¹é½ç ”ç©¶', 'summary': 'æ¢è®¨äº†å¤§æ¨¡å‹çš„å®‰å…¨æ€§å’Œä»·å€¼å¯¹é½é—®é¢˜ï¼Œæå‡ºäº†æ–°çš„è®­ç»ƒå’Œå¯¹é½æ–¹æ³•ã€‚'},
    ]
    
    while len(academic_articles) < 8:
        idx = (len(academic_articles) - len(papers_list)) % len(default_academic)
        default = default_academic[idx]
        article = {
            'id': f'academic-{len(academic_articles)+1}',
            'title': default['title'],
            'summary': default['summary'],
            'category': 'ai',
            'tag': 'è®ºæ–‡è§£è¯»',
            'source': 'arXiv',
            'date': datetime.now().strftime('%mæœˆ%dæ—¥'),
            'url': 'https://arxiv.org/list/cs.AI/recent',
            'isHot': False,
            'views': 3500 + len(academic_articles) * 400
        }
        academic_articles.append(article)
    
    return {
        "version": "1.0",
        "lastUpdated": datetime.now().isoformat() + "Z",
        "categories": [
            {
                "id": "hot",
                "name": "AIçƒ­ç‚¹",
                "articles": hot_articles
            },
            {
                "id": "ai",
                "name": "AIå­¦æœ¯",
                "articles": academic_articles
            }
        ]
    }

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ TechInsight Hub - ä¸­æ–‡å†…å®¹ç”Ÿæˆå™¨")
    print("=" * 60)
    print()
    
    # è·å–æ•°æ®
    print("ğŸ”„ è·å–æœ€æ–°AIå†…å®¹...\n")
    
    news_list = fetch_hackernews(limit=10)
    papers_list = fetch_arxiv(limit=8)
    
    # ç”ŸæˆAPI JSON
    api_data = generate_api_json(news_list, papers_list)
    
    # ä¿å­˜
    Path('api').mkdir(exist_ok=True)
    
    with open('api/tech-news.json', 'w', encoding='utf-8') as f:
        json.dump(api_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    with open('daily_content.json', 'w', encoding='utf-8') as f:
        json.dump({
            'news': news_list,
            'papers': papers_list,
            'updated_at': datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)
    
    total = len(api_data['categories'][0]['articles']) + len(api_data['categories'][1]['articles'])
    
    print()
    print("=" * 60)
    print(f"âœ… ç”Ÿæˆå®Œæˆ!")
    print(f"   ğŸ“° AIçƒ­ç‚¹: {len(api_data['categories'][0]['articles'])} æ¡")
    print(f"   ğŸ“„ AIå­¦æœ¯: {len(api_data['categories'][1]['articles'])} ç¯‡")
    print(f"   ğŸ“Š æ€»è®¡: {total} æ¡ä¸­æ–‡å†…å®¹")
    print("=" * 60)

if __name__ == '__main__':
    main()
