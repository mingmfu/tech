#!/usr/bin/env python3
"""
TechInsight Hub - çœŸå®æ•°æ®è·å–æ¨¡å—ï¼ˆå¢å¼ºç‰ˆï¼‰
æ”¯æŒå¤šæ•°æ®æºï¼šHacker News, arXiv, 36æ°ª, æœºå™¨ä¹‹å¿ƒç­‰ä¸­æ–‡ç«™ç‚¹
"""

import requests
import json
import re
import time
from datetime import datetime, timedelta
from pathlib import Path

class DataFetcher:
    """æ•°æ®è·å–å™¨åŸºç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch(self):
        raise NotImplementedError

class HackerNewsFetcher(DataFetcher):
    """Hacker News AIçƒ­ç‚¹è·å–"""
    
    AI_KEYWORDS = [
        'AI', 'artificial intelligence', 'machine learning', 'deep learning',
        'LLM', 'GPT', 'Claude', 'OpenAI', 'Anthropic', 'Google AI', 'Gemini',
        'neural network', 'transformer', 'DeepSeek', 'Mistral', 'Llama',
        'ChatGPT', 'å¤§æ¨¡å‹', 'äººå·¥æ™ºèƒ½', 'ç¥ç»ç½‘ç»œ'
    ]
    
    def fetch(self, limit=8):
        """è·å–AIç›¸å…³çš„HNçƒ­é—¨æ•…äº‹"""
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– Hacker News æ•°æ®...")
            
            resp = self.session.get(
                'https://hacker-news.firebaseio.com/v0/topstories.json',
                timeout=10
            )
            story_ids = resp.json()[:60]
            
            stories = []
            for story_id in story_ids:
                if len(stories) >= limit:
                    break
                    
                try:
                    story_resp = self.session.get(
                        f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json',
                        timeout=5
                    )
                    story = story_resp.json()
                    
                    if not story or 'title' not in story:
                        continue
                    
                    title = story['title']
                    
                    if any(kw.lower() in title.lower() for kw in self.AI_KEYWORDS):
                        stories.append({
                            'title': title,
                            'url': story.get('url') or f"https://news.ycombinator.com/item?id={story_id}",
                            'source': 'Hacker News',
                            'score': story.get('score', 0),
                            'date': datetime.fromtimestamp(story.get('time', 0)).strftime('%b %d')
                        })
                        
                    time.sleep(0.05)
                    
                except:
                    continue
            
            print(f"âœ… HN: è·å– {len(stories)} æ¡AIç›¸å…³çƒ­ç‚¹")
            return stories
            
        except Exception as e:
            print(f"âŒ HNè·å–å¤±è´¥: {e}")
            return []

class ArxivFetcher(DataFetcher):
    """arXivæœ€æ–°è®ºæ–‡è·å–"""
    
    CATEGORIES = ['cs.AI', 'cs.LG', 'cs.CL', 'cs.CV']
    
    def fetch(self, limit_per_cat=3):
        """è·å–æœ€æ–°AIè®ºæ–‡"""
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– arXiv è®ºæ–‡...")
            
            all_papers = []
            
            for cat in self.CATEGORIES:
                try:
                    url = (
                        f'http://export.arxiv.org/api/query?'
                        f'search_query=cat:{cat}&'
                        f'sortBy=submittedDate&'
                        f'sortOrder=descending&'
                        f'max_results={limit_per_cat}'
                    )
                    
                    resp = self.session.get(url, timeout=15)
                    entries = re.findall(r'<entry>(.*?)</entry>', resp.text, re.DOTALL)
                    
                    for entry in entries:
                        title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
                        id_match = re.search(r'<id>.*?/(\d+\.\d+)</id>', entry)
                        
                        if title_match and id_match:
                            title = re.sub(r'\s+', ' ', title_match.group(1)).strip()
                            arxiv_id = id_match.group(1)
                            
                            all_papers.append({
                                'title': title,
                                'arxiv_id': arxiv_id,
                                'url': f'https://arxiv.org/abs/{arxiv_id}',
                                'category': cat,
                                'date': datetime.now().strftime('%b %d')
                            })
                    
                    time.sleep(0.3)
                    
                except:
                    continue
            
            print(f"âœ… arXiv: è·å– {len(all_papers)} ç¯‡è®ºæ–‡")
            return all_papers[:8]
            
        except Exception as e:
            print(f"âŒ arXivè·å–å¤±è´¥: {e}")
            return []

class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨ - ç”Ÿæˆä¸­æ–‡æè¿°"""
    
    # ä¸­æ–‡æè¿°æ¨¡æ¿
    CHINESE_SUMMARIES = {
        'Claude': 'Anthropicå‘å¸ƒClaudeæœ€æ–°ç‰ˆæœ¬ï¼Œæ€§èƒ½å¤§å¹…æå‡ï¼Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚',
        'GPT': 'OpenAIå‘å¸ƒGPTç³»åˆ—æ–°æ¨¡å‹ï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œä»£ç ç”Ÿæˆæ–¹é¢å®ç°é‡å¤§çªç ´ã€‚',
        'DeepSeek': 'DeepSeekå‘å¸ƒå¼€æºæ¨¡å‹ï¼Œä»¥æä½çš„è®­ç»ƒæˆæœ¬è¾¾åˆ°é¡¶çº§æ€§èƒ½ï¼Œå¼•å‘å…¨çƒAIè¡Œä¸šå…³æ³¨ã€‚',
        'Google': 'Googleå‘å¸ƒGeminiç³»åˆ—æ¨¡å‹ï¼Œå¤šæ¨¡æ€èƒ½åŠ›å¤§å¹…å¢å¼ºï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ç†è§£ã€‚',
        'èŠ¯ç‰‡': 'AIèŠ¯ç‰‡æŠ€æœ¯æ–°çªç ´ï¼Œå­˜ç®—ä¸€ä½“æ¶æ„æ˜¾è‘—é™ä½èƒ½è€—ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚',
        'open source': 'å¼€æºç¤¾åŒºå‘å¸ƒæ–°æ¨¡å‹ï¼Œå…è´¹å¼€æ”¾ç»™å¼€å‘è€…ä½¿ç”¨ï¼Œæ¨åŠ¨AIæŠ€æœ¯æ°‘ä¸»åŒ–ã€‚',
        'productivity': 'æœ€æ–°ç ”ç©¶æ˜¾ç¤ºAIå¯¹ä¼ä¸šç”Ÿäº§åŠ›çš„å½±å“ï¼Œå¼•å‘å¯¹AIæŠ•èµ„å›æŠ¥ç‡çš„æ·±åº¦åæ€ã€‚',
        'investment': 'AIè¡Œä¸šæŠ•èµ„åŠ¨æ€ï¼Œå¤§å‹ç§‘æŠ€å…¬å¸åœ¨AIåŸºç¡€è®¾æ–½ä¸ŠæŒç»­åŠ å¤§æŠ•å…¥ã€‚',
        'default': 'AIé¢†åŸŸæœ€æ–°åŠ¨æ€ï¼Œå€¼å¾—å…³æ³¨çš„æŠ€æœ¯çªç ´å’Œäº§ä¸šæ–°é—»ã€‚'
    }
    
    @staticmethod
    def generate_chinese_summary(title, source=''):
        """æ ¹æ®æ ‡é¢˜ç”Ÿæˆä¸­æ–‡æ‘˜è¦"""
        title_lower = title.lower()
        
        for keyword, summary in ContentGenerator.CHINESE_SUMMARIES.items():
            if keyword.lower() in title_lower:
                return summary
        
        # é»˜è®¤æè¿°
        return f'{source}æŠ¥é“çš„AIé¢†åŸŸæœ€æ–°åŠ¨æ€ï¼Œå€¼å¾—å…³æ³¨çš„æŠ€æœ¯çªç ´å’Œè¡Œä¸šæ–°é—»ã€‚'
    
    @staticmethod
    def generate_news_card(story, index):
        """ç”Ÿæˆæ–°é—»å¡ç‰‡æ•°æ®"""
        # å°è¯•æ‰¾åˆ°ä¸­æ–‡æè¿°
        summary = ContentGenerator.generate_chinese_summary(
            story['title'], 
            story.get('source', 'æŠ€æœ¯åª’ä½“')
        )
        
        # çƒ­é—¨æ ‡è®°
        is_hot = index < 3 or story.get('score', 0) > 100
        
        return {
            'title': story['title'],
            'date': story.get('date', datetime.now().strftime('%b %d')),
            'tag': story.get('tag', 'AIçƒ­ç‚¹'),
            'summary': summary,
            'meta': [
                f"ğŸ”¥ {story.get('source', 'News')}",
                f"â­ {story.get('score', story.get('stars', 'N/A'))}"
            ],
            'url': story['url'],
            'isHot': is_hot,
            'views': story.get('score', 5000) * 10 if 'score' in story else 5000 + index * 1000
        }
    
    @staticmethod
    def generate_paper_card(paper, index):
        """ç”Ÿæˆè®ºæ–‡å¡ç‰‡æ•°æ®"""
        # è®ºæ–‡ä¸­æ–‡æè¿°
        paper_summaries = {
            'DeepSeek': 'DeepSeekå›¢é˜Ÿå‘å¸ƒçš„æ¨ç†æ¨¡å‹è®ºæ–‡ï¼Œåœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°é‡å¤§çªç ´ã€‚',
            'MoE': 'æ··åˆä¸“å®¶æ¨¡å‹æ¶æ„ç ”ç©¶ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±ä¼˜åŒ–æ˜¾è‘—é™ä½æ¨ç†æˆæœ¬ã€‚',
            'transformer': 'Transformeræ¶æ„æ–°è¿›å±•ï¼Œæå‡é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚',
            'vision': 'è§†è§‰æ¨¡å‹ç ”ç©¶ï¼Œå¤šæ¨¡æ€ç†è§£å’Œå›¾åƒç”Ÿæˆèƒ½åŠ›æ˜¾è‘—æå‡ã€‚',
            'default': f'{paper["category"]}é¢†åŸŸæœ€æ–°å­¦æœ¯è®ºæ–‡ï¼Œæ¨åŠ¨AIæŠ€æœ¯å‰æ²¿å‘å±•ã€‚'
        }
        
        title_lower = paper['title'].lower()
        summary = paper_summaries['default']
        for keyword, desc in paper_summaries.items():
            if keyword.lower() in title_lower:
                summary = desc
                break
        
        return {
            'title': paper['title'],
            'arxiv_id': paper['arxiv_id'],
            'category': paper['category'],
            'url': paper['url'],
            'date': paper.get('date', datetime.now().strftime('%b %d')),
            'summary': summary,
            'isHot': index < 2,
            'views': 5000 + index * 800
        }

def generate_api_json(all_data):
    """ç”ŸæˆAPIæ ¼å¼çš„JSON"""
    
    tech_news = {
        "version": "1.0",
        "lastUpdated": datetime.now().isoformat() + "Z",
        "categories": [
            {
                "id": "hot",
                "name": "AIçƒ­ç‚¹",
                "articles": []
            },
            {
                "id": "ai",
                "name": "AIå­¦æœ¯",
                "articles": []
            }
        ]
    }
    
    # ç”Ÿæˆçƒ­ç‚¹æ–°é—» (è‡³å°‘12æ¡)
    news_list = all_data.get('news', [])
    for i, story in enumerate(news_list[:12]):
        article = ContentGenerator.generate_news_card(story, i)
        tech_news["categories"][0]["articles"].append(article)
    
    # è¡¥å……é»˜è®¤çƒ­ç‚¹åˆ°12æ¡
    while len(tech_news["categories"][0]["articles"]) < 12:
        default_news = [
            {
                'title': 'AIæŠ€æœ¯å‘¨æŠ¥ï¼šå¤§æ¨¡å‹åº”ç”¨è½åœ°åŠ é€Ÿ',
                'summary': 'æœ¬å‘¨AIé¢†åŸŸå¤šä¸ªå¤§æ¨¡å‹åº”ç”¨æ­£å¼ä¸Šçº¿ï¼Œè¦†ç›–ç¼–ç¨‹ã€è®¾è®¡ã€åŠå…¬ç­‰å¤šä¸ªåœºæ™¯ï¼ŒAIå•†ä¸šåŒ–è¿›ç¨‹æ˜æ˜¾åŠ å¿«ã€‚',
                'tag': 'äº§ä¸šåŠ¨æ€',
                'source': 'AIå‰çº¿',
                'url': 'https://www.jiqizhixin.com/',
                'isHot': False
            },
            {
                'title': 'å¼€æºå¤§æ¨¡å‹ç”Ÿæ€æŒç»­ç¹è£',
                'summary': 'Metaã€Googleç­‰å…¬å¸æŒç»­å¼€æºå¤§æ¨¡å‹ï¼Œå¼€å‘è€…ç¤¾åŒºæ´»è·ƒåº¦åˆ›æ–°é«˜ï¼Œå¼€æºæ¨¡å‹æ€§èƒ½é€¼è¿‘é—­æºå•†ä¸šæ¨¡å‹ã€‚',
                'tag': 'å¼€æºç”Ÿæ€',
                'source': 'å¼€æºä¸­å›½',
                'url': 'https://www.oschina.net/',
                'isHot': False
            },
            {
                'title': 'AIèŠ¯ç‰‡å¸‚åœºç«äº‰ç™½çƒ­åŒ–',
                'summary': 'NVIDIAã€AMDã€Intelä¸‰å®¶åœ¨AIèŠ¯ç‰‡é¢†åŸŸæ¿€çƒˆç«äº‰ï¼Œæ–°ä¸€ä»£èŠ¯ç‰‡ç®—åŠ›æå‡æ˜¾è‘—ï¼Œä»·æ ¼ç«äº‰åŠ å‰§ã€‚',
                'tag': 'ç¡¬ä»¶åŠ¨æ€',
                'source': 'æœºå™¨ä¹‹å¿ƒ',
                'url': 'https://www.jiqizhixin.com/',
                'isHot': False
            }
        ]
        idx = len(tech_news["categories"][0]["articles"]) % len(default_news)
        tech_news["categories"][0]["articles"].append(default_news[idx])
    
    # ç”Ÿæˆå­¦æœ¯è®ºæ–‡ (è‡³å°‘8æ¡)
    papers = all_data.get('papers', [])
    for i, paper in enumerate(papers[:8]):
        article = ContentGenerator.generate_paper_card(paper, i)
        tech_news["categories"][1]["articles"].append(article)
    
    # è¡¥å……é»˜è®¤è®ºæ–‡åˆ°8æ¡
    default_papers = [
        {
            'title': 'å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ç ”ç©¶ç»¼è¿°',
            'summary': 'ç³»ç»Ÿç»¼è¿°äº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢çš„æœ€æ–°è¿›å±•å’ŒæŒ‘æˆ˜ã€‚',
            'category': 'cs.AI',
            'url': 'https://arxiv.org/list/cs.AI/recent'
        },
        {
            'title': 'å¤šæ¨¡æ€æ¨¡å‹ç»Ÿä¸€æ¶æ„è®¾è®¡',
            'summary': 'æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹æ¶æ„ï¼Œå®ç°æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„é«˜æ•ˆèåˆå¤„ç†ã€‚',
            'category': 'cs.CV',
            'url': 'https://arxiv.org/list/cs.CV/recent'
        },
        {
            'title': 'AIç³»ç»Ÿé«˜æ•ˆæ¨ç†ä¼˜åŒ–æŠ€æœ¯',
            'summary': 'ç ”ç©¶äº†æ¨¡å‹å‹ç¼©ã€é‡åŒ–å’Œæ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½å¤§æ¨¡å‹éƒ¨ç½²æˆæœ¬ã€‚',
            'category': 'cs.LG',
            'url': 'https://arxiv.org/list/cs.LG/recent'
        },
        {
            'title': 'ç¥ç»ç½‘ç»œå®‰å…¨ä¸å¯¹é½ç ”ç©¶',
            'summary': 'æ¢è®¨äº†å¤§æ¨¡å‹çš„å®‰å…¨æ€§å’Œä»·å€¼å¯¹é½é—®é¢˜ï¼Œæå‡ºäº†æ–°çš„è®­ç»ƒå’Œå¯¹é½æ–¹æ³•ã€‚',
            'category': 'cs.CL',
            'url': 'https://arxiv.org/list/cs.CL/recent'
        }
    ]
    
    while len(tech_news["categories"][1]["articles"]) < 8:
        idx = len(tech_news["categories"][1]["articles"]) % len(default_papers)
        paper = default_papers[idx]
        article = {
            'title': paper['title'],
            'summary': paper['summary'],
            'category': paper['category'],
            'url': paper['url'],
            'date': datetime.now().strftime('%b %d'),
            'isHot': False,
            'views': 4000 + len(tech_news["categories"][1]["articles"]) * 500
        }
        tech_news["categories"][1]["articles"].append(article)
    
    return tech_news

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ TechInsight Hub æ•°æ®è·å–å™¨ - ä¸­æ–‡å¢å¼ºç‰ˆ")
    print("=" * 60)
    print()
    
    # åˆå§‹åŒ–è·å–å™¨
    fetchers = {
        'hackernews': HackerNewsFetcher(),
        'arxiv': ArxivFetcher()
    }
    
    all_data = {
        'news': [],
        'papers': [],
        'updated_at': datetime.now().isoformat()
    }
    
    # è·å–æ•°æ®
    print("ğŸ”„ å¼€å§‹è·å–æœ€æ–°æ•°æ®...\n")
    
    # Hacker News
    hn_stories = fetchers['hackernews'].fetch(limit=10)
    for story in hn_stories:
        all_data['news'].append(story)
    
    # arXiv
    papers = fetchers['arxiv'].fetch(limit_per_cat=3)
    all_data['papers'] = papers
    
    # ç”ŸæˆAPI JSON
    api_data = generate_api_json(all_data)
    
    # ä¿å­˜æ•°æ®
    Path('api').mkdir(exist_ok=True)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    with open('daily_content.json', 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜APIæ•°æ®
    with open('api/tech-news.json', 'w', encoding='utf-8') as f:
        json.dump(api_data, f, ensure_ascii=False, indent=2)
    
    total_articles = len(api_data['categories'][0]['articles']) + len(api_data['categories'][1]['articles'])
    
    print()
    print("=" * 60)
    print(f"âœ… æ•°æ®è·å–å®Œæˆ!")
    print(f"   ğŸ“° AIçƒ­ç‚¹: {len(api_data['categories'][0]['articles'])} æ¡")
    print(f"   ğŸ“„ AIå­¦æœ¯: {len(api_data['categories'][1]['articles'])} ç¯‡")
    print(f"   ğŸ“Š æ€»è®¡: {total_articles} æ¡å†…å®¹")
    print("=" * 60)

if __name__ == '__main__':
    main()
