#!/usr/bin/env python3
"""
TechInsight Hub - çœŸå®æ•°æ®è·å–æ¨¡å—
æ”¯æŒå¤šä¸ªå…è´¹æ•°æ®æºï¼šHacker News, arXiv, GitHub
"""

import requests
import json
import re
import time
from datetime import datetime, timedelta
from pathlib import Path

class DataFetcher:
    """æ•°æ®è·å–å™¨åŸºç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'TechInsight-Hub/1.0 (Research Bot)'
        })
    
    def fetch(self):
        raise NotImplementedError

class HackerNewsFetcher(DataFetcher):
    """Hacker News AIçƒ­ç‚¹è·å–"""
    
    AI_KEYWORDS = [
        'AI', 'artificial intelligence', 'machine learning', 'deep learning',
        'LLM', 'GPT', 'Claude', 'OpenAI', 'Anthropic', 'Google AI', 'Gemini',
        'neural network', 'transformer', 'DeepSeek', 'Mistral', 'Llama'
    ]
    
    def fetch(self, limit=5):
        """è·å–AIç›¸å…³çš„HNçƒ­é—¨æ•…äº‹"""
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– Hacker News æ•°æ®...")
            
            # è·å–top stories
            resp = self.session.get(
                'https://hacker-news.firebaseio.com/v0/topstories.json',
                timeout=10
            )
            story_ids = resp.json()[:50]  # æ£€æŸ¥å‰50ä¸ª
            
            stories = []
            for story_id in story_ids:
                if len(stories) >= limit:
                    break
                    
                try:
                    story_resp = self.session.get(
                        f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json',
                        timeout=5
                    )
                    story = story_resp.json()
                    
                    if not story or 'title' not in story:
                        continue
                    
                    title = story['title']
                    
                    # æ£€æŸ¥AIå…³é”®è¯
                    if any(kw.lower() in title.lower() for kw in self.AI_KEYWORDS):
                        stories.append({
                            'title': title,
                            'url': story.get('url') or f"https://news.ycombinator.com/item?id={story_id}",
                            'source': 'Hacker News',
                            'score': story.get('score', 0),
                            'comments': story.get('descendants', 0),
                            'date': datetime.fromtimestamp(story.get('time', 0)).strftime('%b %d')
                        })
                        
                    time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    continue
            
            print(f"âœ… HN: è·å– {len(stories)} æ¡AIç›¸å…³çƒ­ç‚¹")
            return stories
            
        except Exception as e:
            print(f"âŒ HNè·å–å¤±è´¥: {e}")
            return []

class ArxivFetcher(DataFetcher):
    """arXivæœ€æ–°è®ºæ–‡è·å–"""
    
    CATEGORIES = ['cs.AI', 'cs.LG', 'cs.CL', 'cs.CV', 'cs.RO']
    
    def fetch(self, limit_per_cat=3):
        """è·å–æœ€æ–°AIè®ºæ–‡"""
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– arXiv è®ºæ–‡...")
            
            all_papers = []
            
            for cat in self.CATEGORIES:
                try:
                    url = (
                        f'http://export.arxiv.org/api/query?'
                        f'search_query=cat:{cat}&'
                        f'sortBy=submittedDate&'
                        f'sortOrder=descending&'
                        f'max_results={limit_per_cat}'
                    )
                    
                    resp = self.session.get(url, timeout=15)
                    
                    # è§£æAtom feed
                    entries = re.findall(r'<entry>(.*?)\u003c/entry>', resp.text, re.DOTALL)
                    
                    for entry in entries:
                        title_match = re.search(r'<title>(.*?)\u003c/title>', entry, re.DOTALL)
                        id_match = re.search(r'<id>.*?/(\d+\.\d+)\u003c/id>', entry)
                        published_match = re.search(r'<published>(.*?)\u003c/published>', entry)
                        
                        if title_match and id_match:
                            title = re.sub(r'\s+', ' ', title_match.group(1)).strip()
                            arxiv_id = id_match.group(1)
                            
                            # æ¸…ç†æ ‡é¢˜
                            title = title.replace('\n', ' ')
                            
                            all_papers.append({
                                'title': title,
                                'arxiv_id': arxiv_id,
                                'url': f'https://arxiv.org/abs/{arxiv_id}',
                                'category': cat,
                                'date': datetime.now().strftime('%b %d')
                            })
                    
                    time.sleep(0.5)  # å°Šé‡arXivé€Ÿç‡é™åˆ¶
                    
                except Exception as e:
                    print(f"   âš ï¸ {cat} è·å–å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… arXiv: è·å– {len(all_papers)} ç¯‡è®ºæ–‡")
            return all_papers[:10]  # è¿”å›å‰10ç¯‡
            
        except Exception as e:
            print(f"âŒ arXivè·å–å¤±è´¥: {e}")
            return []

class GitHubFetcher(DataFetcher):
    """GitHubçƒ­é—¨AIé¡¹ç›®è·å–"""
    
    def fetch(self, limit=3):
        """è·å–GitHub Trending AIé¡¹ç›®"""
        try:
            print("ğŸ“¡ æ­£åœ¨è·å– GitHub æ•°æ®...")
            
            # æœç´¢æœ€è¿‘ä¸€å‘¨åˆ›å»ºçš„AIé¡¹ç›®ï¼ŒæŒ‰staræ’åº
            last_week = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            
            queries = [
                'machine learning stars:>100',
                'deep learning stars:>100',
                'LLM stars:>50',
                'AI tool stars:>50'
            ]
            
            all_repos = []
            
            for query in queries[:2]:  # é™åˆ¶æŸ¥è¯¢æ•°é‡é¿å…é¢‘ç‡é™åˆ¶
                try:
                    url = (
                        f'https://api.github.com/search/repositories?'
                        f'q={requests.utils.quote(query)}+created:>{last_week}&'
                        f'sort=stars&order=desc&per_page=5'
                    )
                    
                    resp = self.session.get(url, timeout=10)
                    
                    if resp.status_code == 200:
                        data = resp.json()
                        for item in data.get('items', []):
                            all_repos.append({
                                'title': f"{item['full_name']}",
                                'description': item.get('description', 'No description') or 'No description',
                                'url': item['html_url'],
                                'stars': item['stargazers_count'],
                                'language': item.get('language', 'Unknown'),
                                'source': 'GitHub',
                                'date': datetime.now().strftime('%b %d')
                            })
                    
                    time.sleep(1)  # GitHub APIé¢‘ç‡é™åˆ¶
                    
                except Exception as e:
                    continue
            
            # å»é‡å¹¶æŒ‰starsæ’åº
            seen = set()
            unique_repos = []
            for repo in sorted(all_repos, key=lambda x: x['stars'], reverse=True):
                if repo['url'] not in seen:
                    seen.add(repo['url'])
                    unique_repos.append(repo)
            
            print(f"âœ… GitHub: è·å– {len(unique_repos[:limit])} ä¸ªé¡¹ç›®")
            return unique_repos[:limit]
            
        except Exception as e:
            print(f"âŒ GitHubè·å–å¤±è´¥: {e}")
            return []

class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_news_card(story, index):
        """ç”Ÿæˆæ–°é—»å¡ç‰‡HTML"""
        tags = ['çƒ­ç‚¹è®¨è®º', 'å¼€æºé¡¹ç›®', 'ç ”ç©¶çªç ´', 'äº§å“å‘å¸ƒ', 'äº§ä¸šåŠ¨æ€', 'æŠ€æœ¯è¶‹åŠ¿']
        tag = tags[index % len(tags)]
        
        return {
            'title': story['title'],
            'date': story.get('date', datetime.now().strftime('%b %d')),
            'tag': tag,
            'summary': story.get('description', f"æ¥è‡ª{story.get('source', 'Unknown')}çš„æœ€æ–°åŠ¨æ€"),
            'meta': [
                f"ğŸ”¥ {story.get('source', 'News')}",
                f"â­ {story.get('score', story.get('stars', 'N/A'))}"
            ],
            'url': story['url']
        }
    
    @staticmethod
    def generate_paper_card(paper):
        """ç”Ÿæˆè®ºæ–‡å¡ç‰‡HTML"""
        return {
            'title': paper['title'],
            'arxiv_id': paper['arxiv_id'],
            'category': paper['category'],
            'url': paper['url'],
            'date': paper.get('date', datetime.now().strftime('%b %d'))
        }

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ TechInsight Hub æ•°æ®è·å–å™¨")
    print("=" * 60)
    print()
    
    # åˆå§‹åŒ–è·å–å™¨
    fetchers = {
        'hackernews': HackerNewsFetcher(),
        'arxiv': ArxivFetcher(),
        'github': GitHubFetcher()
    }
    
    all_data = {
        'news': [],
        'papers': [],
        'updated_at': datetime.now().isoformat()
    }
    
    # è·å–æ•°æ®
    print("ğŸ”„ å¼€å§‹è·å–æœ€æ–°æ•°æ®...\n")
    
    # Hacker News
    hn_stories = fetchers['hackernews'].fetch(limit=3)
    for i, story in enumerate(hn_stories):
        all_data['news'].append(ContentGenerator.generate_news_card(story, i))
    
    # GitHub
    repos = fetchers['github'].fetch(limit=2)
    for i, repo in enumerate(repos):
        all_data['news'].append(ContentGenerator.generate_news_card(repo, i + 3))
    
    # arXiv
    papers = fetchers['arxiv'].fetch(limit_per_cat=2)
    all_data['papers'] = [ContentGenerator.generate_paper_card(p) for p in papers[:5]]
    
    # ä¿å­˜æ•°æ®
    output_file = Path('daily_content.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print(f"âœ… æ•°æ®è·å–å®Œæˆ!")
    print(f"   ğŸ“° æ–°é—»: {len(all_data['news'])} æ¡")
    print(f"   ğŸ“„ è®ºæ–‡: {len(all_data['papers'])} ç¯‡")
    print(f"   ğŸ’¾ ä¿å­˜è‡³: {output_file}")
    print("=" * 60)

if __name__ == '__main__':
    main()
